# -*- coding: utf-8 -*-
"""main_file.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ekmxc6k-KftZFdvlU4AovW8c9m8gcbaB
"""

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision.transforms as transforms
import torch.utils.data as data
import numpy as np
import random
import math
import json
import ast
import datetime
from datetime import timedelta

from google.colab import drive
drive.mount('/content/gdrive')

# ===========================================================
# ================= Data Collecting Class ===================
# ===========================================================
class data_collect():
    def __init__(self, normalize_function = 2, normalize_company = 0, num_range = 1, train_size = 70, 
                 val_size = 15, test_size = 15, start_point_diff = 25, start_point_deviation = 5, 
                 length = 25, pred_length = 5, company_group = True, random_batch = False, shuffle_test = False,
                 no_change_range = 1.70, data_points = range(0,14), start_date = None, end_date = None, label_type = 0):
        self.normalize_function = normalize_function
        self.normalize_company = normalize_company
        self.num_range = num_range
        self.train_size = train_size
        self.val_size = val_size
        self.test_size = test_size
        self.start_point_diff = start_point_diff
        self.start_point_deviation = start_point_deviation  
        self.length = length
        self.pred_length = pred_length
        self.no_change_range = no_change_range
        self.company_group = company_group
        self.random_batch = random_batch
        self.shuffle_test = shuffle_test
        self.data_points = data_points
        self.label_type = label_type
        self.start_date = start_date
        self.end_date = end_date


    def format_data(self, data, filename):
        # Generate fake data for testing if no data is given
        if (data == None):  
            # For fake data, assume dimension size
            num_data_per_day = 20
            num_companies = 160
            num_days = 830
            data = random_input_gen(num_data_per_day, num_companies, num_days)
        elif (data == True):
            # Read data from .json file
            data = read_from_database_to_list(self.data_points, filename, self.start_date, self.end_date)
            # Get length of data input
            num_data_per_day = len(data[0][0])
            num_companies = len(data[0])
            num_days = len(data)
        
        
        # Add Labels
        data = add_labels(data, self.normalize_function, self.normalize_company, self.num_range, self.pred_length, self.no_change_range, self.data_points, self.label_type)

        # Return batched the data (3 tensors)
        return batch(data, self.train_size, self.val_size, self.test_size, self.start_point_diff, self.start_point_deviation, self.length, self.pred_length, self.company_group, self.random_batch, self.shuffle_test)

    def get_label_frequency(self, data, dataset_type):
        data_loader = torch.utils.data.DataLoader(data, batch_size=1)
        zeros = 0
        ones = 0
        half = 0
        for data_item, data_label in data_loader:
            label_to_count = data_label.tolist()
            if (self.label_type == 0):
                if (self.company_group):
                    for i in label_to_count:
                        for j in i:
                            if (j == 0.0):
                                zeros += 1
                            elif (j == 0.5):
                                half += 1
                            elif (j == 1.0):
                                ones += 1
                else:
                    for i in label_to_count:
                        if (i == 0.0):
                            zeros += 1
                        elif (i == 0.5):
                            half += 1
                        elif (i == 1.0):
                            ones += 1
                        else:
                            print(i)
            elif (self.label_type == 1):
                if (self.company_group):
                    for i in label_to_count:
                        for j in i:
                            if (j == 0):
                                zeros += 1
                            elif (j == 1):
                                half += 1
                            elif (j == 2):
                                ones += 1
                else:
                    for i in label_to_count:
                        if (i == 0):
                            zeros += 1
                        elif (i == 1):
                            half += 1
                        elif (i == 2):
                            ones += 1
        print("0.0: " + str(zeros))
        print("0.5: " + str(half))
        print("1.0: " + str(ones))
        print(dataset_type.upper() + " SET LABEL FREQUENCY")
        if (zeros + ones + half != 0):
            print("Percent of 0.0 label: ", zeros / (zeros + ones + half))
            print("Percent of 0.5 label: ", half / (zeros + ones + half))
            print("Percent of 1.0 label: ", ones / (zeros + ones + half))
        else:
            print("Empty Dataset")
        return None

# ======================================================
# ============== Investment Account Class ==============
# ======================================================

class investment_account():
    def __init__(self, data_dict, price_dict, list_companies, start_capital = 5000, start_period = "2019-01-09", 
                end_period = "2019-08-01", num_top_stock = 3, ind_transaction_cost = 7.00, 
                investment_change_frequency = 7):  # Who really needs a test set huh      

        self.data_dict = data_dict     # To give forward pass data (logarithmically normalized) -- Companies in alphabetical order
        self.price_dict = price_dict   # To compare the prices
        self.list_companies = list_companies   # list of companies in alphabetical order
        self.capital = start_capital
        self.start_period = start_period
        self.end_period = end_period
        self.num_top_stock = num_top_stock
        self.ind_transaction_cost = ind_transaction_cost
        self.investment_change_frequency = investment_change_frequency

        self.stocks = [[None] * 3 for i in range(self.num_top_stock)] # 3-list. First item is date, second item is company, third item is number of stocks
        self.date = datetime.date(int(start_period[0:4]), int(start_period[5:7]), int(start_period[8:10])) # The time step's current progress
        self.iteration = 0 # Iteration number 
        
        print("==== START INFORMATION ====")
        print("Starting Capital: " + str(self.capital))
        print("Starting Date: " + str(self.start_period))
        print("Ending Date: " + str(self.end_period))
        print("Invest in top " + str(self.num_top_stock) + " stocks")
        print("===========================")

    def time_step(self, model):                # MOST IMPORTANT FUNCTION
        self.iteration += 1
        self.update_stocks()
        self.sell_stocks()
        self.buy_stocks(model)
        value = self.capital
        for stock in self.stocks:
            value += stock[2] * self.price_dict[str(stock[0])][stock[1]]
        print("Current Worth: " + str(value))
        print("Current Date: " + str(self.date))
        print("Currently Held Stocks: " + str(self.stocks))
        if (self.date == datetime.date(int(self.end_period[0:4]), int(self.end_period[5:7]), int(self.end_period[8:10]))):
            return True
        return False
    
    def update_stocks(self):
        for stock in self.stocks:
                stock[0] = self.date
        return True

    def sell_stocks(self):
        if (self.stocks[0][1] != None):
                for stock in self.stocks:
                        self.capital += stock[2] * self.price_dict[str(stock[0])][stock[1]]
        return True

    def buy_stocks(self, model): # ASSUME DATA IS ALREADY LOGARITHMICALLY NORMALIZED
        top_stocks = [[None] * 3 for i in range(self.num_top_stock)]
        predictions_tensor = model(self.data_dict[str(self.date)].cuda())
        predictions_list = predictions_tensor.tolist()[0]
        prediction_print_list = []
        predictions_dict = {}
        highest_pred_company = []
        num_same_stocks = 0
        top_companies = []
        
        for i in range(len(self.list_companies)):
            prediction_print_list.append(self.list_companies[i] + ": " + str(predictions_list[i]))
        print(prediction_print_list)
        
        print(predictions_list)
        
        for symbol_count in range(len(self.list_companies)):
            predictions_dict[self.list_companies[symbol_count]] = predictions_list[symbol_count]
            
        for count in range(self.num_top_stock):
            highest_pred_company += [self.list_companies[0]]
            for symbol in self.list_companies:
                if (predictions_dict[symbol] > predictions_dict[highest_pred_company[count]] and symbol not in top_companies):
                    highest_pred_company[count] = symbol
            top_companies += [highest_pred_company[count]]
            for stock in range(self.num_top_stock):
                if (self.stocks[stock][1] == highest_pred_company[count]):
                    num_same_stocks += 1
            top_stocks[count][0] = str(self.date)
            top_stocks[count][1] = highest_pred_company[count]
        
        self.capital -= self.ind_transaction_cost * (3 - num_same_stocks)
        
        for count in range(self.num_top_stock):
            top_stocks[count][2] = math.floor((self.capital/(self.num_top_stock - count)) / self.price_dict[str(self.date)][highest_pred_company[count]])
            self.capital -= top_stocks[count][2] * self.price_dict[str(self.date)][highest_pred_company[count]]
        
        self.stocks = top_stocks
        self.date += timedelta(days = self.investment_change_frequency)
        
        return True

# ===========================================================
# ================= Random input generator ==================
# ===========================================================

# 9 input per day       --> dim 1
# 165 companies         --> dim 2
# ~763 days             --> dim 3

def random_input_gen(num_data_per_day = 20, num_companies = 160, num_days = 830, data_points = range(0,14)): # random numbers ranging from +- 100,000,000,000 (100 billion)
    data = [[[None for k in range(num_data_per_day)] for j in range(num_companies)] for i in range(num_days)]
    for i in range(num_days):
        for j in range(num_companies):
            for k in range(num_data_per_day):
                data[i][j][k] = random.randint(-100000000000,100000000001)
    # Force numbers to ensure logarithmic normalization works
    data[0][0][0] = 15
    data[0][0][1] = 0.05
    return data

# ============================================================
# ===== Formatting data from .json to dictionary to list =====
# ============================================================

# data_points = list    --> Contains all indexes that are desired
#                                   ~~~ RATIOS ~~~
#                       --> 0 = EPS, 1 = PE ratio, 2 = PPS,
#                       --> 3 = asset turnover 4 = cash flow,
#                       --> 5 = current ratio, 6 = return on equity,
#                       --> 7 = working capital
#                                 ~~~ Stock Data ~~~
#                       --> 8 = Closing stock price,
#                       --> 9 = 14-day moving avg,
#                       --> 10 = 37-day moving average

def read_from_database_to_list(data_points = range(0,14), filename = "data_list_complete.json", start_date = None, end_date = None):      # Take in raw str, and convert to dictionary, then to list

        with open(filename, "r") as f:              # *******FILE LOCATION/NAME MAY DIFFER ACCORDING TO YOUR REQUIREMENTS******
                content = f.readlines()
        content = [x.strip() for x in content] 
        data = []
        for i in range(0,len(content)):
                data.append(ast.literal_eval(content[i]))       # Converting to dictionary

        data = data[0]
        
        list_companies = list(data["2019-07-17"].keys())        # All company symbols (anyday works after april 14th 2019)
        list_companies.sort(reverse = False)                    # Sort alphabetically company names

        list_days = list(data.keys())                           # All database days
        list_days = sorted(list_days, key = sorting)            # Sort days in past -> future order
        
        if (start_date != None):
                list_days = list_days[list_days.index(start_date):]
        if (end_date != None):
                list_days = list_days[:list_days.index(end_date) + 1]
                
        bad_companies = ["WLKP", "VMC", "VRS", "REX", "USLM"]   # These companies data are incomplete. To be removed (not enough data) --> I ADDED "USLM" TO MAKE IT WORK ???
        bad_days = []                                           # List of incomplete days (mostly comprised of days before april 14th 2019)

        for day in list_days:                                   # Delete bad days and companies
                num_companies = len(data[day])
                for company in list_companies:
                        if (company in bad_companies and company in data[day]):
                                del data[day][company]
                        if (num_companies < 160):
                                del data[day]
                                bad_days.append(day)
                                break

        for company in bad_companies:                           # Remove bad companies from company list
                if(company in list_companies):
                        list_companies.remove(company)

        for day in bad_days:                                    # Remove bad days from days list
                list_days.remove(day)
        
        # Give AGX data for April 14th (accidentally omitted)
        # data["2016-04-14"]["AGX"] = data["2016-04-15"]["AGX"]

        num_days = len(list_days)                                    # Dimensions of data
        num_companies = len(list_companies)
        num_data_points = len(data_points)


        list_data = [[[0 for i in range(num_data_points)] for j in range(num_companies)] for k in range(num_days)]

        # Input all data to premade 3D list in order    
        counter_i = 0
        counter_j = 0
        counter_k = 0

        for day in list_days:
                for company in list_companies:
                        for data_point in data_points:
                                list_data[counter_i][counter_j][counter_k] = data[day][company][data_point]
                                counter_k += 1
                        counter_k = 0
                        counter_j += 1
                counter_j = 0
                counter_i += 1
        return list_data            # Type list with all requested data (Formatted same as day/company/data_point)

def sorting(L):                     # Function to sort the days in chronological order
        splitup = L.split('-')
        return splitup[0], splitup[1], splitup[2]

# ===========================================================
# ====================== Normalization ======================
# ===========================================================

# normalize_function = 0    --> No normalization                (Baseline model did not train)
# normalize_function = 1    --> linear normalization            (Baseline model did not train)
# normalize_function = 2    --> logarithmic normalization       (Only function that works for baseline model)

# normalize_company = 0     --> normalize inside each company   (Only normalization that works for baseline model)
# normalize_company = 1     --> normalize across all companies  (Did not work for baseline model)

# range = 0                 --> no limit
# range = 1                 --> [-1, 1]
# range = 2                 --> [ 0, 1]

def normalize_data(data, normalize_function = 0, normalize_company = 0, num_range = 0):
    
    # Simple error checking
    if ((num_range != 0 and num_range != 1 and num_range != 2) or (normalize_company != 0 and normalize_company != 1) or (normalize_function != 0 and normalize_function != 1 and normalize_function != 2)):
        print("invalid input")
        return False

    if ((range == 1 or range == 2) and normalize_function == 0):
        print("cannot limit range with no normalization")
        return False
    
    if (range == 0 and normalize_function != 0):
        print("must limit range when normalizing data")
        return False
    
    num_days = len(data)                # Get data dimensions
    num_companies = len(data[0])
    num_data_per_day = len(data[0][0])

    if (normalize_function == 2):       # Take the log 10 of all numbers if we perform logarithmic normalization, else proceed to obtain max/min values
        for i in range(num_days):
            for j in range(num_companies):
                for k in range(num_data_per_day):
                    data[i][j][k] = log_normalization(data[i][j][k])
    
    # 2D list to record each max/min values for each input variable type (the 8 ratios and 3 stock price data)
    maxmin_values = [[0.0,0.0] for i in range(num_data_per_day)]
    div_by_zero_count = 0
    if (normalize_company == 0 and normalize_function != 0):
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    if (maxmin_values[k][1] < data[j][i][k]):
                        maxmin_values[k][1] = data[j][i][k]
                    if (maxmin_values[k][0] > data[j][i][k]):
                        maxmin_values[k][0] = data[j][i][k]
                    
            # Normalize within a single company
            for j in range(num_days):
                for k in range(num_data_per_day):
                    try:
                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])
                    except:
                        div_by_zero_count += 1
                        '''
                        print("Error: DIVISION BY ZERO")
                        print("day number:     ", j)
                        print("company number: ", i)
                        print("data index:     ", k)
                        '''

    elif (normalize_company == 1 and normalize_function != 0):
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    if (maxmin_values[k][1] < data[j][i][k]):
                        maxmin_values[k][1] = data[j][i][k]
                    if (maxmin_values[k][0] > data[j][i][k]):
                        maxmin_values[k][0] = data[j][i][k]
                    
        # Normalize for all data across all companies
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    try:
                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])
                    except:
                        div_by_zero_count += 1
                        '''
                        print("Error: DIVISION BY ZERO")
                        print("day number:     ", j)
                        print("company number: ", i)
                        print("data index:     ", k)
                        '''
    #print("Num zero data points: " + str(div_by_zero_count))
    # Normalize the range of the data
    if (num_range == 1):
        for i in range(num_days):
            for j in range(num_companies):
                for k in range(num_data_per_day):
                    data[i][j][k] = (data[i][j][k] - 0.5) * 2.0

    # Return normalize data of type list
    return data

def log_normalization(x):   # Log 10 normalization function
    if (x < 0):
        return -1 * math.log10((-1 * x) + 1)
    else:
        return math.log10(x + 1)
      
def normalize_data_dict(data, normalize_function = 0, normalize_company = 0, num_range = 0, data_points = range(0,14)):
    with open(filename, "r") as f:              # *******FILE LOCATION/NAME MAY DIFFER ACCORDING TO YOUR REQUIREMENTS******
            content = f.readlines()
    content = [x.strip() for x in content] 
    data = []
    for i in range(0,len(content)):
            data.append(ast.literal_eval(content[i]))       # Converting to dictionary

    data = data[0]
    if (get_dict):
        return data
    list_companies = list(data["2019-07-17"].keys())        # All company symbols (anyday works after april 14th 2019)
    list_companies.sort(reverse = False)                    # Sort alphabetically company names

    list_days = list(data.keys())                           # All database days
    list_days = sorted(list_days, key = sorting)            # Sort days in past -> future order

    if (start_date != None):
        list_days = list_days[list_days.index(start_date):]
    if (end_date != None):
        list_days = list_days[:list_days.index(end_date)]

    bad_companies = ["WLKP", "VMC", "VRS", "REX", "USLM"]   # These companies data are incomplete. To be removed (not enough data) --> I ADDED "USLM" TO MAKE IT WORK ???
    bad_days = []                                           # List of incomplete days (mostly comprised of days before april 14th 2019)

    for day in list_days:                                   # Delete bad days and companies
        num_companies = len(data[day])
        for company in list_companies:
            if (company in bad_companies and company in data[day]):
                del data[day][company]
            if (num_companies < 160):
                del data[day]
                bad_days.append(day)
                break

    for company in bad_companies:                           # Remove bad companies from company list
        if(company in list_companies):
            list_companies.remove(company)

    for day in bad_days:                                    # Remove bad days from days list
        list_days.remove(day)

    # Give AGX data for April 14th (accidentally omitted)
    # data["2016-04-14"]["AGX"] = data["2016-04-15"]["AGX"]


    num_days = len(list_days)                                    # Dimensions of data
    num_companies = len(list_companies)
    num_data_points = len(data_points)
        
    # Simple error checking
    if ((num_range != 0 and num_range != 1 and num_range != 2) or (normalize_company != 0 and normalize_company != 1) or (normalize_function != 0 and normalize_function != 1 and normalize_function != 2)):
        print("invalid input")
        return False

    if ((range == 1 or range == 2) and normalize_function == 0):
        print("cannot limit range with no normalization")
        return False
    
    if (range == 0 and normalize_function != 0):
        print("must limit range when normalizing data")
        return False

    if (normalize_function == 2):       # Take the log 10 of all numbers if we perform logarithmic normalization, else proceed to obtain max/min values
        for i in range(num_days):
            for j in range(num_companies):
                for k in range(num_data_points):
                    data[i][j][k] = log_normalization(data[i][j][k])
    
    # 2D list to record each max/min values for each input variable type (the 8 ratios and 3 stock price data)
    maxmin_values = [[0.0,0.0] for i in range(num_data_points)]
    div_by_zero_count = 0
    if (normalize_company == 0 and normalize_function != 0):
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    if (maxmin_values[k][1] < data[j][i][k]):
                        maxmin_values[k][1] = data[j][i][k]
                    if (maxmin_values[k][0] > data[j][i][k]):
                        maxmin_values[k][0] = data[j][i][k]
                    
            # Normalize within a single company
            for j in range(num_days):
                for k in range(num_data_per_day):
                    try:
                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])
                    except:
                        div_by_zero_count += 1
                        '''
                        print("Error: DIVISION BY ZERO")
                        print("day number:     ", j)
                        print("company number: ", i)
                        print("data index:     ", k)
                        '''

    elif (normalize_company == 1 and normalize_function != 0):
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    if (maxmin_values[k][1] < data[j][i][k]):
                        maxmin_values[k][1] = data[j][i][k]
                    if (maxmin_values[k][0] > data[j][i][k]):
                        maxmin_values[k][0] = data[j][i][k]
                    
        # Normalize for all data across all companies
        for i in range(num_companies):
            for j in range(num_days):
                for k in range(num_data_per_day):
                    try:
                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])
                    except:
                        div_by_zero_count += 1
                        '''
                        print("Error: DIVISION BY ZERO")
                        print("day number:     ", j)
                        print("company number: ", i)
                        print("data index:     ", k)
                        '''
    #print("Num zero data points: " + str(div_by_zero_count))
    # Normalize the range of the data
    if (num_range == 1):
        for i in range(num_days):
            for j in range(num_companies):
                for k in range(num_data_per_day):
                    data[i][j][k] = (data[i][j][k] - 0.5) * 2.0

    # Return normalize data of type list
    return data


# ===========================================================================
# ========================= Labeling and Formatting =========================
# ===========================================================================

# Label = 1  increase in price
# Label = 0  no change in price (within percent error margin no_change_range)
# Label = -1 decrease in price

def add_labels(data, normalize_function = 2, normalize_company = 0, num_range = 1, pred_length = 5, no_change_range = 1.70, data_points = range(0,14), label_type = 0):
    # Get dimensions of data
    num_days = len(data)
    num_companies = len(data[0])

    # Get closing price's index
    if (8 not in data_points):
        print("Data does not contain closing stock price")
    price_index = data_points.index(8)
    
    # Label each input with the price 5 working days later
    labeled_data = []
    labeled_data_by_day =[]
    for i in range(num_days):
        for j in range(num_companies):
            if (num_days - i > pred_length):     # We can give a label
                if (data[i][j][price_index] > data[i + pred_length][j][price_index]*(1+(no_change_range/100.0))):     # Increase in price
                    label = 1.0
                elif (data[i][j][price_index] < data[i + pred_length][j][price_index]*(1-(no_change_range/100.0))):   # Decrease in price
                    label = 0.0
                else:                           # No change in price (range of allowance)
                    label = 0.5
            else:                               # We cannot give a label (no price 5 days ahead available yet)
                label = None
            if (label != None and label_type == 1):
                label = int(label*2.0)
            labeled_data_by_day.append(label)
        labeled_data.append(labeled_data_by_day)
        labeled_data_by_day = []
    
    # Normalize the data
    normalized_data = normalize_data(data, normalize_function, normalize_company, num_range)

    # Add labels to the normalized data
    formatted_data = []
    for day in range(len(normalized_data)):
        formatted_day_data = []
        for company in range(len(normalized_data[day])):
            formatted_day_data.append([normalized_data[day][company], labeled_data[day][company]])
        formatted_data.append(formatted_day_data)
    
    # Return type list with labels attached
    return formatted_data

# ============================================================
# ========================= Batching =========================
# ============================================================

# random = False    --> means first 70% months are training, next 15% are validation
#                       and next 15% are test (assuming percentages did not change)
# random = True     --> randomly pick 70% of months, 15% of months, and 15% of months for 
#                       train/val/test data

# start_point_diff  --> how far apart are the first days in adjacent batchs

# train_size, val_size, test_size   --> percentage of batches to be in each set (must add to 100%)

# start_point_deviation --> the start_point can deviate +- 5 (for example) from the original start_point_diff it was set for

# length        --> how many days in one "item". Days being the days where the market is open for trade

# company_group = True  --> all 165 companies at once
# company_group = False --> 1 company at a time

def batch(data, train_size = 70, val_size = 15, test_size = 15, start_point_diff = 25, start_point_deviation = 5, length = 25, pred_length = 5, company_group = True, random_batch = False, shuffle_test = False):
    # Ensure percentages add up properly
    if (train_size + val_size + test_size != 100):
        print("ensure train_size + val_size + test_size = 100%")
        return None, None, None

    num_days = len(data)
    num_companies = len(data[0])

    # Use start_point_diff, start_point_deviation, and length to get the day stamps we will cover
    times = []
    time_start = 0
    while((time_start + length + pred_length) < num_days):  # Note that the case where we deal with individual companies, we still give the same start date for each batch item
        times.append(time_start)
        time_start = time_start + start_point_diff
    
    for time in range(0,len(times)):
        if (times[time] != 0):
            times[time] += random.randint(-start_point_deviation,start_point_deviation)
   
    # Batching the data
    # Batch data according to start_point_diff, start_point_deviation, length, and company_group
    batched_data = []
    if (company_group):                                    # All 200 companies at once (for finding relationship between companies)
        for i in times:                                    # Iterate through start days
            single_batch = []
            price_label = []
            for day in range(i, i + length):               # Iterate through the length of data per item
                count = 0
                single_day = []
                for companies in range(num_companies):
                    for ratios in data[day][companies]:    # Iterate through all companies
                        if (type(ratios) is list):
                            single_day.append(ratios)
                            count += 1
                        elif ((type(ratios) is int or type(ratios) is float) and day == i + length - 1):
                            price_label.append(ratios)
                single_batch.append(single_day)
            if (price_label != []):
                batched_data.append([single_batch,price_label])
    else:                                                   # One company at a time (no relationship between companies)
        for i in times:                                     # Iterate through start days
            for companies in range(num_companies):          # Iterate through the length of data per item
                price_label = None
                single_company = []
                for day in range(i, i + length):
                    for ratios in data[day][companies]:    # Iterate through all companies
                        if (type(ratios) is list):
                            single_company.append(ratios)
                        elif ((type(ratios) is int or type(ratios) is float) and day == i + length - 1):
                            price_label = ratios
                if (price_label != None):
                    batched_data.append([single_company,price_label])
        
    train_val_split = int(len(batched_data)*train_size/100.0)
    val_test_split = int(len(batched_data)*(train_size + val_size)/100.0)
    
    if (random_batch):
        if (shuffle_test == False): 
            test = batched_data[val_test_split:]
            batched_data = batched_data[:val_test_split]
            random.shuffle(batched_data)    
            train = batched_data[:train_val_split]
            val = batched_data[train_val_split:]
        else:
            random.shuffle(batched_data)    
            train = batched_data[:train_val_split]
            val = batched_data[train_val_split:val_test_split]
            test = batched_data[val_test_split:]
    else:    
        train = batched_data[:train_val_split]
        val = batched_data[train_val_split:val_test_split]
        test = batched_data[val_test_split:]

    train_data = data_to_tensor(train, company_group)
    val_data = data_to_tensor(val, company_group)
    test_data = data_to_tensor(test, company_group)
    
    # Return 3 tensors
    return train_data, val_data, test_data

def data_to_tensor(batched_data, company_group):
    tensor_data = []
    if (company_group):
      for i in batched_data:
        item_tuple = (torch.FloatTensor(i[0]),torch.as_tensor(i[1]))
        tensor_data.append(item_tuple)
    else:
      for i in batched_data:
        item_tuple = (torch.FloatTensor(i[0]),torch.as_tensor(i[1]))
        tensor_data.append(item_tuple)
    # Return data tensor and label tensor together in a tuple
    return tensor_data

def get_companies_and_days(filename = "data_list_complete.json", start_date = None, end_date = None):      # Take in raw str, and convert to dictionary, then to list

        with open(filename, "r") as f:              # *******FILE LOCATION/NAME MAY DIFFER ACCORDING TO YOUR REQUIREMENTS******
                content = f.readlines()
        content = [x.strip() for x in content] 
        data = []
        for i in range(0,len(content)):
                data.append(ast.literal_eval(content[i]))       # Converting to dictionary

        data = data[0]
        
        list_companies = list(data["2019-07-17"].keys())        # All company symbols (anyday works after april 14th 2019)
        list_companies.sort(reverse = False)                    # Sort alphabetically company names

        list_days = list(data.keys())                           # All database days
        list_days = sorted(list_days, key = sorting)            # Sort days in past -> future order
        
        if (start_date != None):
                list_days = list_days[list_days.index(start_date):]
        if (end_date != None):
                list_days = list_days[:list_days.index(end_date) + 1]
                
        bad_companies = ["WLKP", "VMC", "VRS", "REX", "USLM"]   # These companies data are incomplete. To be removed (not enough data) --> I ADDED "USLM" TO MAKE IT WORK ???
        bad_days = []                                           # List of incomplete days (mostly comprised of days before april 14th 2019)

        for day in list_days:                                   # Delete bad days and companies
                num_companies = len(data[day])
                for company in list_companies:
                        if (company in bad_companies and company in data[day]):
                                del data[day][company]
                        if (num_companies < 160):
                                del data[day]
                                bad_days.append(day)
                                break

        for company in bad_companies:                           # Remove bad companies from company list
                if(company in list_companies):
                        list_companies.remove(company)

        for day in bad_days:                                    # Remove bad days from days list
                list_days.remove(day)
            
        return list_days, list_companies
      
def get_data_dict(data_list_normalized, days):
        data_dict = {}
        for count in range(len(days)):
                data_dict[days[count]] = torch.FloatTensor(data_list_normalized[count])
        return data_dict
      
def get_price_dict(price_list, days, companies):
  price_dict = {}
  for count_day in range(len(days)):
    price_dict_day = {}
    for count_company in range(len(companies)):
      price_dict_day[companies[count_company]] = price_list[count_day][count_company][0]
    price_dict[days[count_day]] = price_dict_day
  return price_dict

# ===================================================================
# ==================== EXPLANATION OF PARAMETERS ====================
# ===================================================================

# ------ Data Dimensions ------
# 9 input per day       --> dim 1 (if we only want to do stock prices, we can extract it ourselves)
# 165 companies         --> dim 2
# ~763 days             --> dim 3
 
# ------ Data input ------
# data = None       (randomly generate between -100 billion and 100 billion)
# data = Some_list  (normal input)

# ------ Normalization ------
# normalize_function = 0    (no normalization)
# normalize_function = 1    (linear normalization) --> Horibly failed for random case, prob for real case too
# normalize_function = 2    (logarithmic normalization)

# normalize_company = 0     (normalize within each company)
# normalize_company = 1     (normalize across all companies)

# num_range = 0             (no normalization range)
# num_range = 1             (normalization range [-1,1])
# num_range = 2             (normalization range [0,1])

# ------ Batching ------
# start_point_diff = m          (m working days between two adjacent batch items) --> Influences size of train/val/test dataset
#                               ex. start_point_diff = 25                                   -->     start_days = [0,25,50,75,100,...]
# start_point_deviation = n     (+- n days for deviation from the evenly split start points. Cannot go below 0.)
#                               ex. start_point_diff = 25, start_point_deviation = 5        -->     start_days = [2,24,55,70,100...]
# pred_length = x               (x days worth of data history per item)
# company_group = True          Batch items by all companies in one set. Will produce a label of size [num_companies]   
#                               -->     less train/val/test data (but potentially learns company relations)
# comapny_group = False         Batch items by individual company. Will produce a label of size 1
#                               -->     more train/val/test data (but no company relations)
# random = True                 split train/val/test set randomly (no chronological order)
# random = False                split train/val/test set chrnologically (train fist, then validation, then test)

# ------ Labeling ------
# no_change_range = x           (+-x% deviation is to be considered that the price has not changed)
# label_type = 0                One classification --> MSELoss or BCEWithLogitsLoss
# label_type = 1                Many classifications --> CrossEntropyLoss
# ~below are not parameters, but just the label in the data~
# label = 0                     (decrease in price)
# label = 0.5                   (no change in price, within the error margin no_change_range)
# label = 1                     (increase in price)
# Note: depending on company_group = True / False, the label may be a single number after each batch item, or a 1D-list of size num_companies

# ========================================
# =============== Get Data ===============
# ========================================

# True means get data from .json database. None means use random data
# can use, for example data_single_company.get_label_frequency(train_data_single, "Train") to print label frequencies (return True)

filename = "/content/gdrive/My Drive/Colab Notebooks/Stock Price Direction Prediction/Data/data_list_complete.json"

# Data for a all companies

data_all_company =  data_collect(normalize_function = 2, 
                                 normalize_company = 0, 
                                 num_range = 2,
                                 train_size = 100, 
                                 val_size = 0, 
                                 test_size = 0, 
                                 start_point_diff = 1, 
                                 start_point_deviation = 0, 
                                 length = 25, 
                                 pred_length = 5, 
                                 company_group = True, 
                                 random_batch = True, 
                                 shuffle_test = False,
                                 no_change_range = 1.70, # BEST VALUE IS 1.70 --> for pred_length = 5
                                 data_points = range(0,14), # ALWAYS INCLUDE VARIABLE #8
                                 start_date = "2016-04-15",
                                 end_date = "2019-01-04",
                                 label_type = 0)

train_data_all, val_data_all, test_data_all = data_all_company.format_data(True, filename)

# ===============================================================
# =============== Accuracy and training functions ===============
# ===============================================================

torch.manual_seed(1000)

def train_all(model, train_data, val_data, batch_size=16, num_epochs=100, lr = 0.0001, company_group = False, shuffle = False, label_type = 0, weight_decay = 0, print_out = True):
    
    criterion = nn.MSELoss()
    optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay = weight_decay)
    iters, losses, val_losses, train_acc, val_acc = [], [], [], [], []
    
    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = shuffle)
    if (val_data != []):
        val_loader = torch.utils.data.DataLoader(val_data, batch_size = batch_size, shuffle = shuffle)
    # training
    n = 0 
    for epoch in range(num_epochs):
        for data, labels in iter(train_loader):
            out = model(data).cuda()
            if (company_group == False):
                if (label_type == 0):
                    labels = labels.unsqueeze(1).cuda()
            if (company_group):
                if (label_type == 1):
                    out = out.squeeze(1).cuda()
                    labels = labels.long().squeeze(1).cuda()
            labels = labels.cuda()
            loss = criterion(out, labels)
            loss.backward()             
            optimizer.step()              
            optimizer.zero_grad()

        # save training info
        iters.append(n)
        losses.append(float(loss)/batch_size) 
        if (val_data != []):
            for data, labels in iter(val_loader):
                out = model(data).cuda()
                if (company_group == False):
                    if (label_type == 0):
                        labels = labels.unsqueeze(1).cuda()
                if (company_group):
                    if (label_type == 1):
                        labels = labels.long().cuda()
                labels = labels.cuda()
                loss = criterion(out, labels)
            val_losses.append(float(loss)/batch_size)
        
        # Training and Validation Accuracy
        train_acc.append(get_accuracy_all(model, train_data, batch_size = batch_size, label_type = label_type))
        if (val_data != []):
            val_acc.append(get_accuracy_all(model, val_data, batch_size = batch_size, label_type = label_type))
        n += 1
        # Output Accuracy for each epoch
        if (print_out):
            if (val_data != []):
                print("Epoch: ",(epoch + 1), "    Train Loss: ", losses[n-1],"      Val Loss: ", val_losses[n-1],"    Train Accuracy: ", train_acc[n-1], "     Validation Accuracy: ", val_acc[n-1])
            else:
                print("Epoch: ",(epoch + 1), "    Train Loss: ", losses[n-1],"      Train Accuracy: ", train_acc[n-1])
                

    # plotting
    if (print_out):
        plt.title("Training Curve")
        plt.plot(iters, losses, label="Train")
        if (val_data != []):
            plt.plot(iters, val_losses, label = "Val")
        plt.xlabel("Iterations")
        plt.ylabel("Loss")
        plt.legend(loc = 'best')
        plt.show()

        plt.title("Training Curve")
        plt.plot(iters, train_acc, label="Train")
        if (val_data != []):
            plt.plot(iters, val_acc, label="Validation")
        plt.xlabel("Iterations")
        plt.ylabel("Training Accuracy")
        plt.legend(loc='best')
        plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    if (val_data != []):
        print("Final Validation Accuracy: {}".format(val_acc[-1]))  

def get_accuracy_all(model, data, batch_size = 16, label_type = 0, print_out = False):
    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)
    correct = 0
    total = 0
    if (label_type == 0):
        for data, labels in data_loader:
            output = model(data).cuda() # Forward Pass
            labels = labels.view(-1,160).cuda()                     # CHANGED FOR ALL MODEL
            if (print_out):
              label_out = labels.squeeze(0).tolist()
              output_out = F.relu(output).squeeze(0).tolist()
              for i in range(len(label_out)):
                for j in range(len(label_out[i])):
                  print ("Label: ", label_out[i][j], "  Pred: ", output_out[i][j])
            correct += compare_pred_all(labels, output)
            total += len(labels[0]) * len(labels)
    elif (label_type == 1):
        for data, labels in data_loader:
            output = model(data).cuda() # Forward Pass
            labels = labels.view(-1,160).cuda()
            pred = output.max(1, keepdim=True)[1] # Max Log-Probability
            correct += pred.eq(labels.view_as(pred)).sum().item()
            total += len(labels[0]) * len(labels)
    return correct / total

def compare_pred_all(labels, pred):
    list_labels = labels.tolist()
    list_pred = pred.tolist()
    correct = 0
    for i in range(len(list_labels)):
      for j in range(len(list_labels[i])):
        if (list_labels[i][j] == 1.0 and list_pred[i][j] >= 0.67):
          correct += 1
        elif (list_labels[i][j] == 0.5 and (list_pred[i][j] < 0.67 and list_pred[i][j] > 0.33)):
          correct += 1
        elif (list_labels[i][j] == 0.0 and list_pred[i][j] <= 0.33):
          correct += 1
    return correct

# ======================================
# =============== Models ===============
# ======================================

class neuralnet_all_company(nn.Module):    # BEST MODEL
    def __init__(self, data_points, length):
      super(neuralnet_all_company, self).__init__()
      self.conv1 = nn.Conv1d(in_channels=14, out_channels=22, kernel_size=3, padding = 1)
      self.conv2 = nn.Conv1d(in_channels=22, out_channels=30, kernel_size=5, padding = 2)
      self.pool1 = nn.MaxPool1d(5,5)
      self.pool2 = nn.MaxPool1d(4,4)
      self.fc1 = nn.Linear(30 * 200, 720)
      self.fc2 = nn.Linear(720, 320)
      self.fc3 = nn.Linear(320, 160)
      self.length = length
      self.data_points = data_points

    def forward(self, x):
      x = x.view(-1, self.data_points, self.length * 160).cuda()
      x = self.pool1(F.relu(self.conv1(x))).cuda()
      x = self.pool2(F.relu(self.conv2(x))).cuda()
      x = x.view(-1, 30 * 200).cuda()
      x = F.relu(self.fc1(x)).cuda()
      x = F.relu(self.fc2(x)).cuda()
      x = self.fc3(x).cuda()
      return x

# ==================================================
# ==================== Training ====================
# ==================================================
data_all_company.get_label_frequency(train_data_all, "TRAINING")
print("\n")
data_all_company.get_label_frequency(val_data_all, "VALIDATION")
print("\n")
data_all_company.get_label_frequency(test_data_all, "TEST")

length = 25                      #### May change if you change it in data formatting
num_data = 14                    #### May change if you chose to not include all data
model_all = []
model_all = neuralnet_all_company(num_data, length).cuda()

train_all(model_all, train_data_all, val_data_all, batch_size=20, num_epochs=500, lr = 0.0002, shuffle = True, company_group = True, label_type = 0)

test_accuracy = get_accuracy_all(model_all, test_data_all, 
             batch_size = 128, label_type = 0, print_out = False)

print("Final Test Accuracy: ", test_accuracy*100, "%")

# ==================================================
# ============ MONEY SIMULATION TESTING ============
# ==================================================

filename = "/content/gdrive/My Drive/Colab Notebooks/Stock Price Direction Prediction/Data/data_list_complete.json"

end = False  
first_date = "2016-04-15"
sim_start_date = "2019-01-09"        # Starts a wednesday, ends on wednesday
sim_end_date = "2019-06-19"          # 28 time steps (28 weeks)

days, companies = get_companies_and_days(filename, start_date = None, end_date = sim_end_date)

length = 25                      #### May change if you change it in data formatting
num_data = 14                    #### May change depending on how much data you want to include

data_all_company =  data_collect(normalize_function = 2, 
                                 normalize_company = 0, 
                                 num_range = 2,
                                 train_size = 100, 
                                 val_size = 0, 
                                 test_size = 0, 
                                 start_point_diff = 1, 
                                 start_point_deviation = 0, 
                                 length = 25, 
                                 pred_length = 5, 
                                 company_group = True, 
                                 random_batch = True, 
                                 shuffle_test = False,
                                 no_change_range = 1.70,
                                 data_points = range(0,14),
                                 start_date = None,
                                 end_date = None,
                                 label_type = 0)
    
train_data_all, val_data_all, test_data_all = data_all_company.format_data(True, filename) 

print("Successfully created labeled data")

# Creating data_dict (normalized data)
train_data_dict = {}
train_loader = torch.utils.data.DataLoader(train_data_all, batch_size = 1, shuffle = False)
count = 0
for data, label in train_loader:
    try:
        train_data_dict[days[count]] = data
    except:
        break
    count += 1

print("Successfully created data dictionary")

# Creating price_dict (not normalized)
price_list = read_from_database_to_list(data_points = [8], filename = filename, start_date = None, end_date = sim_end_date)
price_dict = get_price_dict(price_list, days, companies)

print("Successfully price dictionary")

curr_date = datetime.date(int(sim_start_date[0:4]), int(sim_start_date[5:7]), int(sim_start_date[8:10]))

my_account = investment_account(train_data_dict, price_dict, companies, start_capital = 5000, 
                                start_period = sim_start_date, end_period = sim_end_date, 
                                num_top_stock = 3, ind_transaction_cost = 7.00, 
                                investment_change_frequency = 7)

print("Successfully created investment account")

print("========== STARTING SIMULATION ==========")
iteration = 1

while(end == False):
  
    print("ITERATION #" + str(iteration))
    
    iteration += 1
    
    train_data_curr = train_data_all[:days.index(str(curr_date)) + 1]
    val_data_curr = []
    
    model_all = None
    model_all = neuralnet_all_company(num_data, length).cuda()
    
    train_all(model_all, train_data_curr, val_data_curr, batch_size=20, num_epochs=400, lr = 0.0002, shuffle = True, company_group = True, label_type = 0, print_out = False)

    end = my_account.time_step(model_all) # GO TO NEXT DAY (1 week ahead)
    
    curr_date = curr_date + timedelta(weeks = 1)
    
print("====== FINAL RESULTS ======")
my_account.update_stocks()
my_account.sell_stocks()
print("End Capital: " + str(my_account.capital))
print("Start Date: " + sim_start_date)
print("End Date: " + sim_end_date)

# VIEW ALL COMPANIES, AND WHICH INCREASED THE MOST (by percentage)
list_increase_mult = []
print(days)
for company in companies:
  increase_mult =  price_dict["2019-01-22"][company] / price_dict["2019-01-14"][company]
  list_increase_mult += [[company, increase_mult]]
  
for i in range(len(list_increase_mult)):
  for j in range(len(list_increase_mult)-i-1):
    if (list_increase_mult[j][1] > list_increase_mult[j+1][1]):
      list_increase_mult[j], list_increase_mult[j+1] = list_increase_mult[j+1], list_increase_mult[j]

for i in list_increase_mult:
  print(i)

# VIEW SINGLE COMPANY AND ITS PROGRESS THROUGH THE DAYS

single_company = "ORN"
reference_day = "2019-01-04"
single_company_prices = []
reference_value = price_dict[reference_day][single_company]
reference_value_list = []
for day in days:
  single_company_prices.append(price_dict[day][single_company])
  
for day in days:
  reference_value_list.append(reference_value)
  
# Plot
plt.title("Price Chart")
plt.plot(range(0,len(days)), single_company_prices, label=single_company)
plt.plot(range(0,len(days)), reference_value_list, label="reference value")
plt.plot(days.index(reference_day), reference_value, 'ys', label=reference_day,  markersize = 4)
plt.xlabel("Days")
plt.ylabel("Prices (USD)")
plt.legend(loc = 'best')
plt.show()

reference_day = "2019-01-09"
for single_company in companies:
  
  reference_value = price_dict[reference_day][single_company]
  single_company_prices = []
  reference_value_list = []
  
  for day in days:
    single_company_prices.append(price_dict[day][single_company])
    
  for day in days:
    reference_value_list.append(reference_value)
  
'''  
plt.title("Price Chart")
plt.plot(range(0,len(days)), single_company_prices, label=single_company)
plt.plot(range(0,len(days)), reference_value_list, label="reference value")
plt.plot(days.index(reference_day), reference_value, 'ys', label=reference_day,  markersize = 4)
plt.xlabel("Days")
plt.ylabel("Prices (USD)")
plt.legend(loc = 'best')
plt.show()
'''

def loss_recovery_statistics(days, companies, price_dict, lower_limit = 0.93, upper_limit = 1.05, pred_length = 5, start_date = "2016-04-15", end_date = "2019-06-19", max_hold_days = 0): 
  # FINDINGS: (my default upper/lower limit are random atm)
  
  # number --> number of days to achieve the status
  # status -->   -1 = failed to hit lower limit and forever remained below    (deprecated)
  #        -->    0 = hit lower limit after *number* days                     (deprecated)
  #        -->    1 = increase in price after a pred_length worth of working days                          (above upper limit)
  #        -->    2 = was initially between upper/lower limit (after pred_length days) but hit upper limit (hit upper limit)
  #        -->    3 = didn't hit upper limit, but sold it at the original price after *number* days        (returned to lower limit)
  #        -->    4 = sold straight away after breaking the "lower_limit" barrier                          (below lower limit, sold ASAP)
  #        -->    5 = never was above upper limit, nor below upper limit, nor hit upper/lower limit        (Hit max days, just sell on last day)
  
  # result in format [number, status] (except for status = 3, have third item in list to indicate percent decrease in price)
  
  # Things to add --> Max hold day (just sell the stock if it status = 5 for *number* days)
  
  success_dict = {}
  days = days[days.index(start_date):days.index(end_date)] # limit number of days to look into
  for day in days[:-1*pred_length]:
    single_day_dict = {}
    for single_company in companies:
      reference_value = price_dict[reference_day][single_company]
      count = pred_length
      if (price_dict[days[days.index(day) + 5]][single_company] >= price_dict[day][single_company] * upper_limit):                  # Above upper limit after pred_length days
        single_day_dict[single_company] = [pred_length, 1, price_dict[days[days.index(day) + 5]][single_company] / price_dict[day][single_company]]
      elif (price_dict[days[days.index(day) + 5]][single_company] <= price_dict[day][single_company] * lower_limit):                # Below lower limit after pred_length days    
        single_day_dict[single_company] = [pred_length, 4, price_dict[days[days.index(day) + 5]][single_company] / price_dict[day][single_company]]
      else:
        for future_day in days[days.index(day)+pred_length:]:
          if (price_dict[future_day][single_company] >= price_dict[day][single_company] * upper_limit):                             # Hit upper limit after being in between upper/lower limit
            single_day_dict[single_company] = [count, 2, price_dict[future_day][single_company] / price_dict[day][single_company]]
            break
          elif (price_dict[future_day][single_company] <= price_dict[day][single_company] * lower_limit):
            single_day_dict[single_company] = [count, 3, price_dict[future_day][single_company] / price_dict[day][single_company]]  # Hit lower limit after being in between upper/lower limit
            break
          count += 1
      if (single_company not in single_day_dict):                                                                                   # Never went above upper limit, below upper limit, nor hit upper/lower limit (hit max days, just sell)
        single_day_dict[single_company] = [count, 5, price_dict[days[-1*pred_length]][single_company] / price_dict[day][single_company]]
        
    success_dict[day] = single_day_dict
  return success_dict

# Test to see if function works on default values
success_statistics_dict = loss_recovery_statistics(days, companies, price_dict)
print("Complete function")

# TESTING FOR BEST UPPER AND LOWER LIMIT VALUES 
# Currently holding lower limit stationary
# Compared by seeing which would yield the most money over a span of 3 year


list_nums = {"1": [], "2": [], "3": [], "4": [], "5": []}
ultimate_gain = {}
start_upper = 100
end_upper = 116
start_lower = 85
end_lower = 101
for upper_limit_count in range(start_upper, end_upper):
  set_gains_dict = {}
  for lower_limit_count in range(start_lower, end_lower):
    total = 0
    ind_gain = 0.0
    lower_limit_percent = lower_limit_count / 100.0
    upper_limit_percent = upper_limit_count / 100.0
    success_statistics_dict = loss_recovery_statistics(days = days, companies = companies, price_dict = price_dict, lower_limit = lower_limit_percent, upper_limit = upper_limit_percent)
    count_nums = {"1": 0, "2": 0, "3": 0, "4": 0, "5": 0}

    for day in days[:-6]:
      for company in companies:
        count_nums[str(success_statistics_dict[day][company][1])] += 1
        ind_gain += success_statistics_dict[day][company][2] / success_statistics_dict[day][company][0]
        total += 1
    for i in range(1,6): # Count frequency of status 1 through 5
      list_nums[str(i)].append(count_nums[str(i)])
    set_gains_dict[str(lower_limit_count)] = ind_gain / total
    print(str(upper_limit_count) + "-" + str(lower_limit_count))
  ultimate_gain[str(upper_limit_count)] = set_gains_dict

#print("Completed data collection for lower limit " + str(start) + "-" + str(end-1) + "%")

plt.title("Stock Statuses")
#plt.plot(range(50,100), list_nums[-1], label="total loss")
#plt.plot(range(50,100), list_nums[0], label="partial loss")
plt.plot(range(start_lower,end_lower), list_nums["1"], label="over upper")
plt.plot(range(start_lower,end_lower), list_nums["2"], label="recovered upper")
plt.plot(range(start_lower,end_lower), list_nums["3"], label="recovered lower")
plt.plot(range(start_lower,end_lower), list_nums["4"], label="under lower")
plt.plot(range(start_lower,end_lower), list_nums["5"], label="in progress")
plt.xlabel("lower limit percentages (%)")
plt.ylabel("Num Stocks")
plt.legend(loc = 'best')
plt.show()

# Useless now
print(list_percent_drop)
money_gain = [] # FINAL MULTIPLIER
for i in range(0,31):
  rating = list_nums["2"][i]*1 + (list_nums["3"][i] * (0.85) * list_percent_drop[i])
  print(str(i + 50) + ": " + str(rating))

# Useful
best_limits = [100,100, 0]
print(ultimate_gain)
for upper_limit_count in range(start_upper, end_upper):
  for lower_limit_count in range(start_lower, end_lower):
    print(str(upper_limit_count) + "-" + str(lower_limit_count) + ": " + str(ultimate_gain[upper_limit_count][str(lower_limit_count)]))
    if (ultimate_gain[upper_limit_count][str(lower_limit_count)] > best_limits[2]):
      best_limits = [str(upper_limit_count), str(lower_limit_count), ultimate_gain[upper_limit_count][str(lower_limit_count)]]
print("THE BEST LIMITS: " + str(best_limits))

lower_limit_percent = 0.90
success_statistics_dict = loss_recovery_statistics(days, companies, price_dict, lower_limit_percent)

record_days = {}

for day in days[:-6]:
  single_day_dict = {}
  for company in companies:
    if (success_statistics_dict[day][company][1] == 0):
      if (success_statistics_dict[day][company][0] not in single_day_dict):
        single_day_dict[success_statistics_dict[day][company][0]] = 1
      else:
        single_day_dict[success_statistics_dict[day][company][0]] += 1
  record_days[day] = single_day_dict
for day in days[:-6]:
  print(day + ": " + str(record_days[day]))