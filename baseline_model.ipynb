{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "497Hr98f7ajn",
        "colab_type": "code",
        "outputId": "3dc0d38a-af39-470c-836b-31a39e42c9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "import ast\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3T_PYYH7dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===========================================================\n",
        "# ================= Data Collecting Class ===================\n",
        "# ===========================================================\n",
        "class data_collect():\n",
        "    def __init__(self, normalize_function = 2, normalize_company = 0, num_range = 1, train_size = 70, \n",
        "                 val_size = 15, test_size = 15, start_point_diff = 25, start_point_deviation = 5, \n",
        "                 length = 25, pred_length = 5, company_group = True, random_batch = False, shuffle_test = False,\n",
        "                 no_change_range = 1.70, data_points = range(0,14), kill_first_percentage = 0, kill_last_percentage = 0, label_type = 0):\n",
        "        self.normalize_function = normalize_function\n",
        "        self.normalize_company = normalize_company\n",
        "        self.num_range = num_range\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.test_size = test_size\n",
        "        self.start_point_diff = start_point_diff\n",
        "        self.start_point_deviation = start_point_deviation  \n",
        "        self.length = length\n",
        "        self.pred_length = pred_length\n",
        "        self.no_change_range = no_change_range\n",
        "        self.company_group = company_group\n",
        "        self.random_batch = random_batch\n",
        "        self.shuffle_test = shuffle_test\n",
        "        self.data_points = data_points\n",
        "        self.label_type = label_type\n",
        "        self.kill_first_percentage = kill_first_percentage\n",
        "        self.kill_last_percentage = kill_last_percentage\n",
        "\n",
        "\n",
        "    def format_data(self, data, filename):\n",
        "        # Generate fake data for testing if no data is given\n",
        "        if (data == None):  \n",
        "            # For fake data, assume dimension size\n",
        "            num_data_per_day = 20\n",
        "            num_companies = 160\n",
        "            num_days = 830\n",
        "            data = random_input_gen(num_data_per_day, num_companies, num_days)\n",
        "        elif (data == True):\n",
        "            # Read data from .json file\n",
        "            data = read_from_database_to_list(self.data_points, filename)\n",
        "            # Get length of data input\n",
        "            num_data_per_day = len(data[0][0])\n",
        "            num_companies = len(data[0])\n",
        "            num_days = len(data)\n",
        "        \n",
        "        \n",
        "        # Add Labels\n",
        "        data = add_labels(data, self.normalize_function, self.normalize_company, self.num_range, self.pred_length, self.no_change_range, self.data_points, self.label_type)\n",
        "\n",
        "        # Return batched the data (3 tensors)\n",
        "        return batch(data, self.train_size, self.val_size, self.test_size, self.start_point_diff, self.start_point_deviation, self.length, self.pred_length, self.company_group, self.random_batch, self.shuffle_test, self.kill_first_percentage, self.kill_last_percentage)\n",
        "\n",
        "    def get_label_frequency(self, data, dataset_type):\n",
        "        data_loader = torch.utils.data.DataLoader(data, batch_size=1)\n",
        "        zeros = 0\n",
        "        ones = 0\n",
        "        half = 0\n",
        "        for data_item, data_label in data_loader:\n",
        "            label_to_count = data_label.tolist()\n",
        "            if (self.label_type == 0):\n",
        "                if (self.company_group):\n",
        "                    for i in label_to_count:\n",
        "                        for j in i:\n",
        "                            if (j == 0.0):\n",
        "                                zeros += 1\n",
        "                            elif (j == 0.5):\n",
        "                                half += 1\n",
        "                            elif (j == 1.0):\n",
        "                                ones += 1\n",
        "                else:\n",
        "                    for i in label_to_count:\n",
        "                        if (i == 0.0):\n",
        "                            zeros += 1\n",
        "                        elif (i == 0.5):\n",
        "                            half += 1\n",
        "                        elif (i == 1.0):\n",
        "                            ones += 1\n",
        "                        else:\n",
        "                            print(i)\n",
        "            elif (self.label_type == 1):\n",
        "                if (self.company_group):\n",
        "                    for i in label_to_count:\n",
        "                        for j in i:\n",
        "                            if (j == 0):\n",
        "                                zeros += 1\n",
        "                            elif (j == 1):\n",
        "                                half += 1\n",
        "                            elif (j == 2):\n",
        "                                ones += 1\n",
        "                else:\n",
        "                    for i in label_to_count:\n",
        "                        if (i == 0):\n",
        "                            zeros += 1\n",
        "                        elif (i == 1):\n",
        "                            half += 1\n",
        "                        elif (i == 2):\n",
        "                            ones += 1\n",
        "        print(\"0.0: \" + str(zeros))\n",
        "        print(\"0.5: \" + str(half))\n",
        "        print(\"1.0: \" + str(ones))\n",
        "        print(dataset_type.upper() + \" SET LABEL FREQUENCY\")\n",
        "        print(\"Percent of 0.0 label: \", zeros / (zeros + ones + half))\n",
        "        print(\"Percent of 0.5 label: \", half / (zeros + ones + half))\n",
        "        print(\"Percent of 1.0 label: \", ones / (zeros + ones + half))\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS9ihZscBjt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===========================================================\n",
        "# ================= Random input generator ==================\n",
        "# ===========================================================\n",
        "\n",
        "# 9 input per day       --> dim 1\n",
        "# 165 companies         --> dim 2\n",
        "# ~763 days             --> dim 3\n",
        "\n",
        "def random_input_gen(num_data_per_day = 20, num_companies = 160, num_days = 830, data_points = range(0,14)): # random numbers ranging from +- 100,000,000,000 (100 billion)\n",
        "    data = [[[None for k in range(num_data_per_day)] for j in range(num_companies)] for i in range(num_days)]\n",
        "    for i in range(num_days):\n",
        "        for j in range(num_companies):\n",
        "            for k in range(num_data_per_day):\n",
        "                data[i][j][k] = random.randint(-100000000000,100000000001)\n",
        "    # Force numbers to ensure logarithmic normalization works\n",
        "    data[0][0][0] = 15\n",
        "    data[0][0][1] = 0.05\n",
        "    return data\n",
        "\n",
        "# ============================================================\n",
        "# ===== Formatting data from .json to dictionary to list =====\n",
        "# ============================================================\n",
        "\n",
        "# data_points = list    --> Contains all indexes that are desired\n",
        "#                                   ~~~ RATIOS ~~~\n",
        "#                       --> 0 = EPS, 1 = PE ratio, 2 = PPS,\n",
        "#                       --> 3 = asset turnover 4 = cash flow,\n",
        "#                       --> 5 = current ratio, 6 = return on equity,\n",
        "#                       --> 7 = working capital\n",
        "#                                 ~~~ Stock Data ~~~\n",
        "#                       --> 8 = Closing stock price,\n",
        "#                       --> 9 = 14-day moving avg,\n",
        "#                       --> 10 = 37-day moving average\n",
        "\n",
        "def read_from_database_to_list(data_points = range(0,14), filename = \"data_list_complete.json\"):      # Take in raw str, and convert to dictionary, then to list\n",
        "\n",
        "        with open(filename, \"r\") as f:              # *******FILE LOCATION/NAME MAY DIFFER ACCORDING TO YOUR REQUIREMENTS******\n",
        "                content = f.readlines()\n",
        "        content = [x.strip() for x in content] \n",
        "        data = []\n",
        "        for i in range(0,len(content)):\n",
        "                data.append(ast.literal_eval(content[i]))       # Converting to dictionary\n",
        "\n",
        "        data = data[0]\n",
        "        list_companies = list(data[\"2019-07-17\"].keys())        # All company symbols (anyday works after april 14th 2019)\n",
        "        list_companies.sort(reverse = False)                    # Sort alphabetically company names\n",
        "\n",
        "        list_days = list(data.keys())                           # All database days\n",
        "        list_days = sorted(list_days, key = sorting)            # Sort days in past -> future order\n",
        "                \n",
        "        bad_companies = [\"WLKP\", \"VMC\", \"VRS\", \"REX\", \"USLM\"]   # These companies data are incomplete. To be removed (not enough data) --> I ADDED \"USLM\" TO MAKE IT WORK ???\n",
        "        bad_days = []                                           # List of incomplete days (mostly comprised of days before april 14th 2019)\n",
        "\n",
        "        for day in list_days:                                   # Delete bad days and companies\n",
        "                num_companies = len(data[day])\n",
        "                for company in list_companies:\n",
        "                        if (company in bad_companies and company in data[day]):\n",
        "                                del data[day][company]\n",
        "                        if (num_companies < 160):\n",
        "                                del data[day]\n",
        "                                bad_days.append(day)\n",
        "                                break\n",
        "\n",
        "        for company in bad_companies:                           # Remove bad companies from company list\n",
        "                if(company in list_companies):\n",
        "                        list_companies.remove(company)\n",
        "\n",
        "        for day in bad_days:                                    # Remove bad days from days list\n",
        "                list_days.remove(day)\n",
        "        \n",
        "        # Give AGX data for April 14th (accidentally omitted)\n",
        "        # data[\"2016-04-14\"][\"AGX\"] = data[\"2016-04-15\"][\"AGX\"]\n",
        "\n",
        "\n",
        "        num_days = len(data)                                    # Dimensions of data\n",
        "        num_companies = len(data[\"2016-04-15\"])\n",
        "        num_data_points = len(data_points)\n",
        "\n",
        "\n",
        "        list_data = [[[0 for i in range(num_data_points)] for j in range(num_companies)] for k in range(num_days)]\n",
        "\n",
        "        # Input all data to premade 3D list in order    \n",
        "        counter_i = 0\n",
        "        counter_j = 0\n",
        "        counter_k = 0\n",
        "\n",
        "        for day in list_days:\n",
        "                for company in list_companies:\n",
        "                        for data_point in data_points:\n",
        "                                list_data[counter_i][counter_j][counter_k] = data[day][company][data_point]\n",
        "                                counter_k += 1\n",
        "                        counter_k = 0\n",
        "                        counter_j += 1\n",
        "                counter_j = 0\n",
        "                counter_i += 1\n",
        "        return list_data            # Type list with all requested data\n",
        "\n",
        "def sorting(L):                     # Function to sort the days in chronological order\n",
        "        splitup = L.split('-')\n",
        "        return splitup[0], splitup[1], splitup[2]\n",
        "\n",
        "# ===========================================================\n",
        "# ====================== Normalization ======================\n",
        "# ===========================================================\n",
        "\n",
        "# normalize_function = 0    --> No normalization                (Baseline model did not train)\n",
        "# normalize_function = 1    --> linear normalization            (Baseline model did not train)\n",
        "# normalize_function = 2    --> logarithmic normalization       (Only function that works for baseline model)\n",
        "\n",
        "# normalize_company = 0     --> normalize inside each company   (Only normalization that works for baseline model)\n",
        "# normalize_company = 1     --> normalize across all companies  (Did not work for baseline model)\n",
        "\n",
        "# range = 0                 --> no limit\n",
        "# range = 1                 --> [-1, 1]\n",
        "# range = 2                 --> [ 0, 1]\n",
        "\n",
        "def normalize_data(data, normalize_function = 0, normalize_company = 0, num_range = 0):\n",
        "    \n",
        "    # Simple error checking\n",
        "    if ((num_range != 0 and num_range != 1 and num_range != 2) or (normalize_company != 0 and normalize_company != 1) or (normalize_function != 0 and normalize_function != 1 and normalize_function != 2)):\n",
        "        print(\"invalid input\")\n",
        "        return False\n",
        "\n",
        "    if ((range == 1 or range == 2) and normalize_function == 0):\n",
        "        print(\"cannot limit range with no normalization\")\n",
        "        return False\n",
        "    \n",
        "    if (range == 0 and normalize_function != 0):\n",
        "        print(\"must limit range when normalizing data\")\n",
        "        return False\n",
        "    \n",
        "    num_days = len(data)                # Get data dimensions\n",
        "    num_companies = len(data[0])\n",
        "    num_data_per_day = len(data[0][0])\n",
        "\n",
        "    if (normalize_function == 2):       # Take the log 10 of all numbers if we perform logarithmic normalization, else proceed to obtain max/min values\n",
        "        for i in range(num_days):\n",
        "            for j in range(num_companies):\n",
        "                for k in range(num_data_per_day):\n",
        "                    data[i][j][k] = log_normalization(data[i][j][k])\n",
        "    \n",
        "    # 2D list to record each max/min values for each input variable type (the 8 ratios and 3 stock price data)\n",
        "    maxmin_values = [[0.0,0.0] for i in range(num_data_per_day)]\n",
        "    div_by_zero_count = 0\n",
        "    if (normalize_company == 0 and normalize_function != 0):\n",
        "        for i in range(num_companies):\n",
        "            for j in range(num_days):\n",
        "                for k in range(num_data_per_day):\n",
        "                    if (maxmin_values[k][1] < data[j][i][k]):\n",
        "                        maxmin_values[k][1] = data[j][i][k]\n",
        "                    if (maxmin_values[k][0] > data[j][i][k]):\n",
        "                        maxmin_values[k][0] = data[j][i][k]\n",
        "                    \n",
        "            # Normalize within a single company\n",
        "            for j in range(num_days):\n",
        "                for k in range(num_data_per_day):\n",
        "                    try:\n",
        "                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])\n",
        "                    except:\n",
        "                        div_by_zero_count += 1\n",
        "                        '''\n",
        "                        print(\"Error: DIVISION BY ZERO\")\n",
        "                        print(\"day number:     \", j)\n",
        "                        print(\"company number: \", i)\n",
        "                        print(\"data index:     \", k)\n",
        "                        '''\n",
        "\n",
        "    elif (normalize_company == 1 and normalize_function != 0):\n",
        "        for i in range(num_companies):\n",
        "            for j in range(num_days):\n",
        "                for k in range(num_data_per_day):\n",
        "                    if (maxmin_values[k][1] < data[j][i][k]):\n",
        "                        maxmin_values[k][1] = data[j][i][k]\n",
        "                    if (maxmin_values[k][0] > data[j][i][k]):\n",
        "                        maxmin_values[k][0] = data[j][i][k]\n",
        "                    \n",
        "        # Normalize for all data across all companies\n",
        "        for i in range(num_companies):\n",
        "            for j in range(num_days):\n",
        "                for k in range(num_data_per_day):\n",
        "                    try:\n",
        "                        data[j][i][k] = (data[j][i][k] - maxmin_values[k][0]) / (maxmin_values[k][1] - maxmin_values[k][0])\n",
        "                    except:\n",
        "                        div_by_zero_count += 1\n",
        "                        '''\n",
        "                        print(\"Error: DIVISION BY ZERO\")\n",
        "                        print(\"day number:     \", j)\n",
        "                        print(\"company number: \", i)\n",
        "                        print(\"data index:     \", k)\n",
        "                        '''\n",
        "    print(\"Num zero data points: \" + str(div_by_zero_count))\n",
        "    # Normalize the range of the data\n",
        "    if (num_range == 1):\n",
        "        for i in range(num_days):\n",
        "            for j in range(num_companies):\n",
        "                for k in range(num_data_per_day):\n",
        "                    data[i][j][k] = (data[i][j][k] - 0.5) * 2.0\n",
        "\n",
        "    # Return normalize data of type list\n",
        "    return data\n",
        "\n",
        "def log_normalization(x):   # Log 10 normalization function\n",
        "    if (x < 0):\n",
        "        return -1 * math.log10((-1 * x) + 1)\n",
        "    else:\n",
        "        return math.log10(x + 1)\n",
        "\n",
        "# ===========================================================================\n",
        "# ========================= Labeling and Formatting =========================\n",
        "# ===========================================================================\n",
        "\n",
        "# Label = 1  increase in price\n",
        "# Label = 0  no change in price (within percent error margin no_change_range)\n",
        "# Label = -1 decrease in price\n",
        "\n",
        "def add_labels(data, normalize_function = 2, normalize_company = 0, num_range = 1, pred_length = 5, no_change_range = 1.70, data_points = range(0,14), label_type = 0):\n",
        "    # Get dimensions of data\n",
        "    num_days = len(data)\n",
        "    num_companies = len(data[0])\n",
        "\n",
        "    # Get closing price's index\n",
        "    if (8 not in data_points):\n",
        "        print(\"Data does not contain closing stock price\")\n",
        "    price_index = data_points.index(8)\n",
        "    \n",
        "    # Label each input with the price 5 working days later\n",
        "    labeled_data = []\n",
        "    labeled_data_by_day =[]\n",
        "    for i in range(num_days):\n",
        "        for j in range(num_companies):\n",
        "            if (num_days - i > pred_length):     # We can give a label\n",
        "                if (data[i][j][price_index] > data[i + pred_length][j][price_index]*(1+(no_change_range/100.0))):     # Increase in price\n",
        "                    label = 1.0\n",
        "                elif (data[i][j][price_index] < data[i + pred_length][j][price_index]*(1-(no_change_range/100.0))):   # Decrease in price\n",
        "                    label = 0.0\n",
        "                else:                           # No change in price (range of allowance)\n",
        "                    label = 0.5\n",
        "            else:                               # We cannot give a label (no price 5 days ahead available yet)\n",
        "                label = None\n",
        "            if (label != None and label_type == 1):\n",
        "                label = int(label*2.0)\n",
        "            labeled_data_by_day.append(label)\n",
        "        labeled_data.append(labeled_data_by_day)\n",
        "        labeled_data_by_day = []\n",
        "    \n",
        "    # Normalize the data\n",
        "    normalized_data = normalize_data(data, normalize_function, normalize_company, num_range)\n",
        "\n",
        "    # Add labels to the normalized data\n",
        "    formatted_data = []\n",
        "    for day in range(len(normalized_data)):\n",
        "        formatted_day_data = []\n",
        "        for company in range(len(normalized_data[day])):\n",
        "            formatted_day_data.append([normalized_data[day][company], labeled_data[day][company]])\n",
        "        formatted_data.append(formatted_day_data)\n",
        "    \n",
        "    # Return type list with labels attached\n",
        "    return formatted_data\n",
        "\n",
        "# ============================================================\n",
        "# ========================= Batching =========================\n",
        "# ============================================================\n",
        "\n",
        "# random = False    --> means first 70% months are training, next 15% are validation\n",
        "#                       and next 15% are test (assuming percentages did not change)\n",
        "# random = True     --> randomly pick 70% of months, 15% of months, and 15% of months for \n",
        "#                       train/val/test data\n",
        "\n",
        "# start_point_diff  --> how far apart are the first days in adjacent batchs\n",
        "\n",
        "# train_size, val_size, test_size   --> percentage of batches to be in each set (must add to 100%)\n",
        "\n",
        "# start_point_deviation --> the start_point can deviate +- 5 (for example) from the original start_point_diff it was set for\n",
        "\n",
        "# length        --> how many days in one \"item\". Days being the days where the market is open for trade\n",
        "\n",
        "# company_group = True  --> all 165 companies at once\n",
        "# company_group = False --> 1 company at a time\n",
        "\n",
        "def batch(data, train_size = 70, val_size = 15, test_size = 15, start_point_diff = 25, start_point_deviation = 5, length = 25, pred_length = 5, company_group = True, random_batch = False, shuffle_test = False, kill_first_percentage = 0, kill_last_percentage = 0):\n",
        "    # Ensure percentages add up properly\n",
        "    if (train_size + val_size + test_size != 100):\n",
        "        print(\"ensure train_size + val_size + test_size = 100%\")\n",
        "        return None, None, None\n",
        "\n",
        "    num_days = len(data)\n",
        "    num_companies = len(data[0])\n",
        "\n",
        "    # Use start_point_diff, start_point_deviation, and length to get the day stamps we will cover\n",
        "    times = []\n",
        "    time_start = 0\n",
        "    while((time_start + length + pred_length) < num_days):  # Note that the case where we deal with individual companies, we still give the same start date for each batch item\n",
        "        times.append(time_start)\n",
        "        time_start = time_start + start_point_diff\n",
        "    \n",
        "    for time in range(0,len(times)):\n",
        "        if (times[time] != 0):\n",
        "            times[time] += random.randint(-pred_length,pred_length)\n",
        "   \n",
        "    # Batching the data\n",
        "    # Batch data according to start_point_diff, start_point_deviation, length, and company_group\n",
        "    batched_data = []\n",
        "    if (company_group):                                    # All 200 companies at once (for finding relationship between companies)\n",
        "        for i in times:                                    # Iterate through start days\n",
        "            single_batch = []\n",
        "            price_label = []\n",
        "            for day in range(i, i + length):               # Iterate through the length of data per item\n",
        "                count = 0\n",
        "                single_day = []\n",
        "                for companies in range(num_companies):\n",
        "                    for ratios in data[day][companies]:    # Iterate through all companies\n",
        "                        if (type(ratios) is list):\n",
        "                            single_day.append(ratios)\n",
        "                            count += 1\n",
        "                        elif ((type(ratios) is int or type(ratios) is float) and day == i + length - 1):\n",
        "                            price_label.append(ratios)\n",
        "                single_batch.append(single_day)\n",
        "            if (price_label != []):\n",
        "                batched_data.append([single_batch,price_label])\n",
        "    else:                                                   # sOne company at a time (no relationship between companies)\n",
        "        for i in times:                                         # Iterate through start days\n",
        "            for companies in range(num_companies):               # Iterate through the length of data per item\n",
        "                price_label = None\n",
        "                single_company = []\n",
        "                for day in range(i, i + length):\n",
        "                    for ratios in data[day][companies]:    # Iterate through all companies\n",
        "                        if (type(ratios) is list):\n",
        "                            single_company.append(ratios)\n",
        "                        elif ((type(ratios) is int or type(ratios) is float) and day == i + length - 1):\n",
        "                            price_label = ratios\n",
        "                if (price_label != None):\n",
        "                    batched_data.append([single_company,price_label])\n",
        "                    \n",
        "    batched_data = batched_data[int(len(batched_data) * (kill_first_percentage)):int(len(batched_data) * (1-kill_last_percentage))]\n",
        "        \n",
        "    train_val_split = int(len(batched_data)*train_size/100.0)\n",
        "    val_test_split = int(len(batched_data)*(train_size + val_size)/100.0)\n",
        "    \n",
        "    if (random_batch):\n",
        "        if (shuffle_test == False): \n",
        "            test = batched_data[val_test_split:]\n",
        "            batched_data = batched_data[:val_test_split]\n",
        "            random.shuffle(batched_data)    \n",
        "            train = batched_data[:train_val_split]\n",
        "            val = batched_data[train_val_split:]\n",
        "        else:\n",
        "            random.shuffle(batched_data)    \n",
        "            train = batched_data[:train_val_split]\n",
        "            val = batched_data[train_val_split:val_test_split]\n",
        "            test = batched_data[val_test_split:]\n",
        "    else:    \n",
        "        train = batched_data[:train_val_split]\n",
        "        val = batched_data[train_val_split:val_test_split]\n",
        "        test = batched_data[val_test_split:]\n",
        "\n",
        "    train_data = data_to_tensor(train, company_group)\n",
        "    val_data = data_to_tensor(val, company_group)\n",
        "    test_data = data_to_tensor(test, company_group)\n",
        "    \n",
        "    # Return 3 tensors\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def data_to_tensor(batched_data, company_group):\n",
        "    tensor_data = []\n",
        "    if (company_group):\n",
        "      for i in batched_data:\n",
        "        item_tuple = (torch.FloatTensor(i[0]),torch.as_tensor(i[1]))\n",
        "        tensor_data.append(item_tuple)\n",
        "    else:\n",
        "      for i in batched_data:\n",
        "        item_tuple = (torch.FloatTensor(i[0]),torch.as_tensor(i[1]))\n",
        "        tensor_data.append(item_tuple)\n",
        "    # Return data tensor and label tensor together in a tuple\n",
        "    return tensor_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1LHZKqNMKp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===================================================================\n",
        "# ==================== EXPLANATION OF PARAMETERS ====================\n",
        "# ===================================================================\n",
        "\n",
        "# ------ Data Dimensions ------\n",
        "# 9 input per day       --> dim 1 (if we only want to do stock prices, we can extract it ourselves)\n",
        "# 165 companies         --> dim 2\n",
        "# ~763 days             --> dim 3\n",
        " \n",
        "# ------ Data input ------\n",
        "# data = None       (randomly generate between -100 billion and 100 billion)\n",
        "# data = Some_list  (normal input)\n",
        "\n",
        "# ------ Normalization ------\n",
        "# normalize_function = 0    (no normalization)\n",
        "# normalize_function = 1    (linear normalization) --> Horibly failed for random case, prob for real case too\n",
        "# normalize_function = 2    (logarithmic normalization)\n",
        "\n",
        "# normalize_company = 0     (normalize within each company)\n",
        "# normalize_company = 1     (normalize across all companies)\n",
        "\n",
        "# num_range = 0             (no normalization range)\n",
        "# num_range = 1             (normalization range [-1,1])\n",
        "# num_range = 2             (normalization range [0,1])\n",
        "\n",
        "# ------ Batching ------\n",
        "# start_point_diff = m          (m working days between two adjacent batch items) --> Influences size of train/val/test dataset\n",
        "#                               ex. start_point_diff = 25                                   -->     start_days = [0,25,50,75,100,...]\n",
        "# start_point_deviation = n     (+- n days for deviation from the evenly split start points. Cannot go below 0.)\n",
        "#                               ex. start_point_diff = 25, start_point_deviation = 5        -->     start_days = [2,24,55,70,100...]\n",
        "# pred_length = x               (x days worth of data history per item)\n",
        "# company_group = True          Batch items by all companies in one set. Will produce a label of size [num_companies]   \n",
        "#                               -->     less train/val/test data (but potentially learns company relations)\n",
        "# comapny_group = False         Batch items by individual company. Will produce a label of size 1\n",
        "#                               -->     more train/val/test data (but no company relations)\n",
        "# random = True                 split train/val/test set randomly (no chronological order)\n",
        "# random = False                split train/val/test set chrnologically (train fist, then validation, then test)\n",
        "\n",
        "# ------ Labeling ------\n",
        "# no_change_range = x           (+-x% deviation is to be considered that the price has not changed)\n",
        "# label_type = 0                One classification --> MSELoss or BCEWithLogitsLoss\n",
        "# label_type = 1                Many classifications --> CrossEntropyLoss\n",
        "# ~below are not parameters, but just the label in the data~\n",
        "# label = 0                     (decrease in price)\n",
        "# label = 0.5                   (no change in price, within the error margin no_change_range)\n",
        "# label = 1                     (increase in price)\n",
        "# Note: depending on company_group = True / False, the label may be a single number after each batch item, or a 1D-list of size num_companies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYuaGRxC7m__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ========================================\n",
        "# =============== Get Data ===============\n",
        "# ========================================\n",
        "\n",
        "# True means get data from .json database. None means use random data\n",
        "# can use, for example data_single_company.get_label_frequency(train_data_single, \"Train\") to print label frequencies (return True)\n",
        "\n",
        "filename = \"/content/gdrive/My Drive/Colab Notebooks/Stock Price Direction Prediction/Data/data_list_complete.json\"\n",
        "#filename = \"/content/gdrive/My Drive/Colab Notebooks/Stock Price Direction Prediction/Data/data_list_complete_random.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc4BquopGcpw",
        "colab_type": "code",
        "outputId": "78a5b42c-eeda-4da9-c337-07a8a0f5185b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Data for a single company\n",
        "\n",
        "                             data =  data_collect(normalize_function = 2,       # Normalization Function\n",
        "                                                                                # 0 = None, 1 = Linear, 2 = Logarithmic\n",
        "                                                  normalize_company = 0,        # Normalization Domain\n",
        "                                                                                # 0 = within each company, 1 = across all companies\n",
        "                                                  num_range = 2,                # Normalization Data Range\n",
        "                                                                                # 0 = no limit, 1 = [-1,1], 2 = [0,1]\n",
        "                                                  train_size = 70,              # Training data percent size of whole dataset\n",
        "                                                  val_size = 15,                # Validation data percent size of whole dataset\n",
        "                                                  test_size = 15,               # Test data percent size of whole dataset\n",
        "                                                  start_point_diff = 1,         # Difference in number of dats between two adjacent\n",
        "                                                                                # days in dataset\n",
        "                                                  start_point_deviation = 0,    # Random deviation (of number of days) from initial\n",
        "                                                                                # start point. Initial start points set by\n",
        "                                                                                # start_point_diff\n",
        "                                                  length = 25,                  # Number of days worth of data in single item       \n",
        "                                                  pred_length = 5,              # How far to look ahead for price direction comparison\n",
        "                                                                                # (unit is in number of trading days)\n",
        "                                                  company_group = True,         # Grouping companies in dataset\n",
        "                                                                                # True = all 160 companies simultaneously (all-companies)\n",
        "                                                                                # False = each company individually (generic model)\n",
        "                                                  random_batch = True,          # Train/val/test ordering\n",
        "                                                                                # (True = randomly, False = chronological)\n",
        "                                                                                # Note that test will remain chronologically at the end\n",
        "                                                                                # if shuffle_test = False\n",
        "                                                  shuffle_test = False,         # True = shuffle the test with train/val\n",
        "                                                                                # False = do not shuffle test with train/val\n",
        "                                                  no_change_range = 1.70,       # +- % allowance to be considered neutral price direction\n",
        "                                                                                # BEST VALUE IS 1.70 (even split)--> for pred_length = 5\n",
        "                                                  data_points = range(0,14),    # Data poitns to include\n",
        "                                                                                # 0-7 represent the 8 ratios, 8 = closing stock price\n",
        "                                                                                # 9 = 14-day moving average, 10 = 37-day moving average\n",
        "                                                                                # 11 = 14-day stochastic moving oscillator\n",
        "                                                                                # 12 = 3-day stochastic moving average\n",
        "                                                                                # 13 = relative strength index\n",
        "                                                  kill_first_percentage = 0.30, # Remove first __ percentage of data\n",
        "                                                  kill_last_percentage = 0.30,  # Remove last __ percentage of data\n",
        "                                                  label_type = 0)               # 0 sets up for MSELoss problem,\n",
        "                                                                                # 1 sets up for Cross Entropy\n",
        "                                                                                # (1 is unavailable for all companies)\n",
        "\n",
        "                             train, val, test = data.format_data(True, filename) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num zero data points: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZKoBt8VysnS",
        "colab_type": "code",
        "outputId": "19af60d7-ec7c-4388-efd3-4f2192ba1a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Data for a all companies\n",
        "\n",
        "data_all_company =  data_collect(normalize_function = 2, \n",
        "                                 normalize_company = 0, \n",
        "                                 num_range = 2,\n",
        "                                 train_size = 80, \n",
        "                                 val_size = 19.5 , \n",
        "                                 test_size = 0.5, \n",
        "                                 start_point_diff = 1, \n",
        "                                 start_point_deviation = 0, \n",
        "                                 length = 25, \n",
        "                                 pred_length = 5, \n",
        "                                 company_group = True, \n",
        "                                 random_batch = True, \n",
        "                                 shuffle_test = False,\n",
        "                                 no_change_range = 1.70, # BEST VALUE IS 1.70 --> for pred_length = 5\n",
        "                                 data_points = range(0,14),\n",
        "                                 kill_first_percentage = 0.15,\n",
        "                                 kill_last_percentage = 0.15,\n",
        "                                 label_type = 0)\n",
        "\n",
        "train_data_all, val_data_all, test_data_all = data_all_company.format_data(True, filename) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num zero data points: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRV64u-715W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===============================================================\n",
        "# =============== Accuracy and training functions ===============\n",
        "# ===============================================================\n",
        "\n",
        "torch.manual_seed(1000) # set random seed\n",
        "\n",
        "def train(model, train_data, val_data, batch_size=16, num_epochs=100, lr = 0.0001, company_group = False, shuffle = False, label_type = 0, weight_decay = 0):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "    iters, losses, val_losses, train_acc, val_acc = [], [], [], [], []\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = shuffle)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size = batch_size, shuffle = shuffle)\n",
        "    # training\n",
        "    n = 0 \n",
        "    for epoch in range(num_epochs):\n",
        "        for data, labels in iter(train_loader):\n",
        "            out = model(data).cuda()\n",
        "            if (company_group == False):\n",
        "                if (label_type == 0):\n",
        "                    labels = labels.unsqueeze(1).cuda()\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()             \n",
        "            optimizer.step()              \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # save training info\n",
        "        iters.append(n)\n",
        "        losses.append(float(loss)/batch_size) \n",
        "        \n",
        "        for data, labels in iter(val_loader):\n",
        "            out = model(data).cuda()\n",
        "            if (company_group == False):\n",
        "                if (label_type == 0):\n",
        "                    labels = labels.unsqueeze(1).cuda()\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(out, labels)\n",
        "        val_losses.append(float(loss)/batch_size)\n",
        "        \n",
        "        # Training and Validation Accuracy\n",
        "        train_acc.append(get_accuracy(model, train_data, batch_size = batch_size, label_type = label_type))\n",
        "        val_acc.append(get_accuracy(model, val_data, batch_size = batch_size, label_type = label_type))\n",
        "        n += 1\n",
        "        # Output Accuracy for each epoch\n",
        "        print(\"Epoch: \",(epoch + 1), \"    Train Loss: \", losses[n-1],\"      Val Loss: \", val_losses[n-1],\"    Train Accuracy: \", train_acc[n-1], \"     Validation Accuracy: \", val_acc[n-1])\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.plot(iters, val_losses, label = \"Val\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))  \n",
        "\n",
        "def get_accuracy(model, data, batch_size = 16, label_type = 0):\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if (label_type == 0):\n",
        "        for data, labels in data_loader:\n",
        "            output = model(data).cuda() # Forward Pass\n",
        "            labels = labels.view(-1,1).cuda()                     # CHANGED FOR ALL MODEL\n",
        "            correct += compare_pred(labels, output)\n",
        "            total += len(labels)\n",
        "    elif (label_type == 1):\n",
        "        for data, labels in data_loader:\n",
        "            output = model(data).cuda() # Forward Pass\n",
        "            labels = labels.view(-1,1).cuda()\n",
        "            pred = output.max(1, keepdim=True)[1] # Max Log-Probability\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += len(labels)\n",
        "    return correct / total\n",
        "\n",
        "def compare_pred(labels, pred):\n",
        "    list_labels = labels.tolist()\n",
        "    list_pred = pred.tolist()\n",
        "    correct = 0\n",
        "    \n",
        "    for i in range(len(list_labels)):\n",
        "      if (list_labels[i][0] == 1.0 and list_pred[i][0] >= 0.67):\n",
        "        correct += 1\n",
        "      elif (list_labels[i][0] == 0.5 and (list_pred[i][0] < 0.67 and list_pred[i][0] > 0.33)):\n",
        "        correct += 1\n",
        "      elif (list_labels[i][0] == 0.0 and list_pred[i][0] <= 0.33):\n",
        "        correct += 1\n",
        "    return correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlxJcXpu-Tj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_all(model, train_data, val_data, batch_size=16, num_epochs=100, lr = 0.0001, company_group = False, shuffle = False, label_type = 0, weight_decay = 0):\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "    iters, losses, val_losses, train_acc, val_acc = [], [], [], [], []\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = shuffle)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size = batch_size, shuffle = shuffle)\n",
        "    # training\n",
        "    n = 0 \n",
        "    for epoch in range(num_epochs):\n",
        "        for data, labels in iter(train_loader):\n",
        "            out = model(data).cuda()\n",
        "            if (company_group == False):\n",
        "                if (label_type == 0):\n",
        "                    labels = labels.unsqueeze(1).cuda()\n",
        "            if (company_group):\n",
        "                if (label_type == 1):\n",
        "                    out = out.squeeze(1).cuda()\n",
        "                    labels = labels.long().squeeze(1).cuda()\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()             \n",
        "            optimizer.step()              \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # save training info\n",
        "        iters.append(n)\n",
        "        losses.append(float(loss)/batch_size) \n",
        "        \n",
        "        for data, labels in iter(val_loader):\n",
        "            out = model(data).cuda()\n",
        "            if (company_group == False):\n",
        "                if (label_type == 0):\n",
        "                    labels = labels.unsqueeze(1).cuda()\n",
        "            if (company_group):\n",
        "                if (label_type == 1):\n",
        "                    labels = labels.long().cuda()\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(out, labels)\n",
        "        val_losses.append(float(loss)/batch_size)\n",
        "        \n",
        "        # Training and Validation Accuracy\n",
        "        train_acc.append(get_accuracy_all(model, train_data, batch_size = batch_size, label_type = label_type))\n",
        "        val_acc.append(get_accuracy_all(model, val_data, batch_size = batch_size, label_type = label_type))\n",
        "        n += 1\n",
        "        # Output Accuracy for each epoch\n",
        "        print(\"Epoch: \",(epoch + 1), \"    Train Loss: \", losses[n-1],\"      Val Loss: \", val_losses[n-1],\"    Train Accuracy: \", train_acc[n-1], \"     Validation Accuracy: \", val_acc[n-1])\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.plot(iters, val_losses, label = \"Val\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))  \n",
        "\n",
        "def get_accuracy_all(model, data, batch_size = 16, label_type = 0, print_out = False):\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if (label_type == 0):\n",
        "        for data, labels in data_loader:\n",
        "            output = model(data).cuda() # Forward Pass\n",
        "            labels = labels.view(-1,160).cuda()                     # CHANGED FOR ALL MODEL\n",
        "            if (print_out):\n",
        "              label_out = labels.squeeze(0).tolist()\n",
        "              output_out = F.relu(output).squeeze(0).tolist()\n",
        "              for i in range(len(label_out)):\n",
        "                for j in range(len(label_out[i])):\n",
        "                  print (\"Label: \", label_out[i][j], \"  Pred: \", output_out[i][j])\n",
        "            correct += compare_pred_all(labels, output)\n",
        "            total += len(labels[0]) * len(labels)\n",
        "    elif (label_type == 1):\n",
        "        for data, labels in data_loader:\n",
        "            output = model(data).cuda() # Forward Pass\n",
        "            labels = labels.view(-1,160).cuda()\n",
        "            pred = output.max(1, keepdim=True)[1] # Max Log-Probability\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += len(labels[0]) * len(labels)\n",
        "    return correct / total\n",
        "\n",
        "def compare_pred_all(labels, pred):\n",
        "    list_labels = labels.tolist()\n",
        "    list_pred = pred.tolist()\n",
        "    correct = 0\n",
        "    for i in range(len(list_labels)):\n",
        "      for j in range(len(list_labels[i])):\n",
        "        if (list_labels[i][j] == 1.0 and list_pred[i][j] >= 0.67):\n",
        "          correct += 1\n",
        "        elif (list_labels[i][j] == 0.5 and (list_pred[i][j] < 0.67 and list_pred[i][j] > 0.33)):\n",
        "          correct += 1\n",
        "        elif (list_labels[i][j] == 0.0 and list_pred[i][j] <= 0.33):\n",
        "          correct += 1\n",
        "    return correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ErAozQV7uKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================\n",
        "# =============== Models ===============\n",
        "# ======================================\n",
        "\n",
        "class neuralnet_single_company_1d(nn.Module):\n",
        "    def __init__(self, data_points, length):\n",
        "        super(neuralnet_single_company_1d, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=20, kernel_size=5, padding = 0)\n",
        "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=25, kernel_size=3, padding = 0)\n",
        "        #self.pool1 = nn.MaxPool1d(2,2)\n",
        "        #self.batch1 = nn.BatchNorm1d(20)\n",
        "        #self.batch2 = nn.BatchNorm1d(25)\n",
        "        #self.batch3 = nn.BatchNorm1d(48)\n",
        "        #self.batch4 = nn.BatchNorm1d(16)\n",
        "        #self.batch5 = nn.BatchNorm1d(3)\n",
        "        self.fc1 = nn.Linear(25 * 14, 48)\n",
        "        self.fc2 = nn.Linear(48, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.length = length\n",
        "        self.data_points = data_points\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.data_points, self.length).cuda()\n",
        "        x = F.relu(self.conv1(x)).cuda()\n",
        "        x = F.relu(self.conv2(x)).cuda()\n",
        "        x = x.view(-1, 25 * 14).cuda()\n",
        "        x = F.relu(self.fc1(x)).cuda()\n",
        "        x = F.relu(self.fc2(x)).cuda()\n",
        "        x = self.fc3(x).cuda()\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqp429FKzHNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class neuralnet_all_company(nn.Module):\n",
        "    def __init__(self, data_points, length):\n",
        "      super(neuralnet_all_company, self).__init__()\n",
        "      self.conv1 = nn.Conv1d(in_channels=14, out_channels=22, kernel_size=3, padding = 1)\n",
        "      self.conv2 = nn.Conv1d(in_channels=22, out_channels=30, kernel_size=5, padding = 2)\n",
        "      self.pool1 = nn.MaxPool1d(5,5)\n",
        "      self.pool2 = nn.MaxPool1d(4,4)\n",
        "      self.fc1 = nn.Linear(30 * 200, 720)\n",
        "      self.fc2 = nn.Linear(720, 320)\n",
        "      self.fc3 = nn.Linear(320, 160)\n",
        "      self.length = length\n",
        "      self.data_points = data_points\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.view(-1, self.data_points, self.length * 160).cuda()\n",
        "      x = self.pool1(F.relu(self.conv1(x))).cuda()\n",
        "      x = self.pool2(F.relu(self.conv2(x))).cuda()\n",
        "      x = x.view(-1, 30 * 200).cuda()\n",
        "      x = F.relu(self.fc1(x)).cuda()\n",
        "      x = F.relu(self.fc2(x)).cuda()\n",
        "      x = self.fc3(x).cuda()\n",
        "      return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EerQsIvK74wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==================================================\n",
        "# ==================== Training ====================\n",
        "# ==================================================\n",
        "data_single_company.get_label_frequency(train_data_single, \"TRAINING\")\n",
        "print(\"\\n\")\n",
        "data_single_company.get_label_frequency(val_data_single, \"VALIDATION\")\n",
        "print(\"\\n\")\n",
        "data_single_company.get_label_frequency(test_data_single, \"TEST\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgEpFxq8Cg8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_data = 14\n",
        "length = 20\n",
        "model_1d = neuralnet_single_company_1d(num_data, length).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daNzUFIKItEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model_1d, train_data_single, val_data_single, batch_size=128, num_epochs= 500, lr = 0.002, company_group = False, shuffle = False, label_type = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK3EEEaZAZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_accuracy(model_1d, test_data_single, \n",
        "             batch_size = 128, label_type = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SzHMHuzQOe",
        "colab_type": "code",
        "outputId": "2f873d75-1108-44f8-bb50-896da00e44fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# ==================================================\n",
        "# ==================== Training ====================\n",
        "# ==================================================\n",
        "data_all_company.get_label_frequency(train_data_all, \"TRAINING\")\n",
        "print(\"\\n\")\n",
        "data_all_company.get_label_frequency(val_data_all, \"VALIDATION\")\n",
        "print(\"\\n\")\n",
        "data_all_company.get_label_frequency(test_data_all, \"TEST\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0: 27964\n",
            "0.5: 28722\n",
            "1.0: 28434\n",
            "TRAINING SET LABEL FREQUENCY\n",
            "Percent of 0.0 label:  0.3285244360902256\n",
            "Percent of 0.5 label:  0.3374295112781955\n",
            "Percent of 1.0 label:  0.33404605263157894\n",
            "\n",
            "\n",
            "0.0: 1058\n",
            "0.5: 1221\n",
            "1.0: 1241\n",
            "VALIDATION SET LABEL FREQUENCY\n",
            "Percent of 0.0 label:  0.30056818181818185\n",
            "Percent of 0.5 label:  0.346875\n",
            "Percent of 1.0 label:  0.35255681818181817\n",
            "\n",
            "\n",
            "0.0: 351\n",
            "0.5: 354\n",
            "1.0: 255\n",
            "TEST SET LABEL FREQUENCY\n",
            "Percent of 0.0 label:  0.365625\n",
            "Percent of 0.5 label:  0.36875\n",
            "Percent of 1.0 label:  0.265625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HaM5PCezY30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 25                      #### May change if you change it in data formatting\n",
        "num_data = 14                    #### May change if you chose to not include all data\n",
        "model_all = []\n",
        "model_all = neuralnet_all_company(num_data, length).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG3RbHqbzoGb",
        "colab_type": "code",
        "outputId": "75b17a29-31bc-40ae-e263-1b9bd3d7ea6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_all(model_all, train_data_all, val_data_all, batch_size=20, num_epochs=500, lr = 0.0002, shuffle = True, company_group = True, label_type = 0) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1     Train Loss:  0.00880034938454628       Val Loss:  0.009737170487642288     Train Accuracy:  0.34207589285714285      Validation Accuracy:  0.34025229357798165\n",
            "Epoch:  2     Train Loss:  0.008364064246416092       Val Loss:  0.007784639298915863     Train Accuracy:  0.3414899553571429      Validation Accuracy:  0.3346330275229358\n",
            "Epoch:  3     Train Loss:  0.008862052112817764       Val Loss:  0.00938783660531044     Train Accuracy:  0.34135044642857143      Validation Accuracy:  0.33446100917431193\n",
            "Epoch:  4     Train Loss:  0.009021944552659988       Val Loss:  0.007838224619626999     Train Accuracy:  0.3435267857142857      Validation Accuracy:  0.3367545871559633\n",
            "Epoch:  5     Train Loss:  0.009814674407243729       Val Loss:  0.008163745701313018     Train Accuracy:  0.34607979910714287      Validation Accuracy:  0.33870412844036696\n",
            "Epoch:  6     Train Loss:  0.008179819583892823       Val Loss:  0.008368942141532897     Train Accuracy:  0.34672154017857143      Validation Accuracy:  0.33904816513761465\n",
            "Epoch:  7     Train Loss:  0.008600400388240814       Val Loss:  0.008149387687444687     Train Accuracy:  0.3464146205357143      Validation Accuracy:  0.33927752293577984\n",
            "Epoch:  8     Train Loss:  0.007970407605171204       Val Loss:  0.009089072793722152     Train Accuracy:  0.34340122767857145      Validation Accuracy:  0.33658256880733944\n",
            "Epoch:  9     Train Loss:  0.007871996611356735       Val Loss:  0.008216845989227294     Train Accuracy:  0.3430385044642857      Validation Accuracy:  0.3361238532110092\n",
            "Epoch:  10     Train Loss:  0.0077141977846622465       Val Loss:  0.008549485355615616     Train Accuracy:  0.3444754464285714      Validation Accuracy:  0.3375573394495413\n",
            "Epoch:  11     Train Loss:  0.008474095165729523       Val Loss:  0.008632075786590577     Train Accuracy:  0.34182477678571427      Validation Accuracy:  0.33469036697247706\n",
            "Epoch:  12     Train Loss:  0.007932256162166595       Val Loss:  0.007981763780117035     Train Accuracy:  0.3445452008928571      Validation Accuracy:  0.3373279816513762\n",
            "Epoch:  13     Train Loss:  0.008039452880620957       Val Loss:  0.00836743861436844     Train Accuracy:  0.34573102678571427      Validation Accuracy:  0.3384747706422018\n",
            "Epoch:  14     Train Loss:  0.007980248332023621       Val Loss:  0.00786677822470665     Train Accuracy:  0.3431640625      Validation Accuracy:  0.33623853211009175\n",
            "Epoch:  15     Train Loss:  0.0075816713273525235       Val Loss:  0.00822371169924736     Train Accuracy:  0.34208984375      Validation Accuracy:  0.33474770642201834\n",
            "Epoch:  16     Train Loss:  0.008089177310466766       Val Loss:  0.008329814672470093     Train Accuracy:  0.34384765625      Validation Accuracy:  0.3368119266055046\n",
            "Epoch:  17     Train Loss:  0.008011773973703385       Val Loss:  0.007977437227964401     Train Accuracy:  0.34468470982142857      Validation Accuracy:  0.33721330275229355\n",
            "Epoch:  18     Train Loss:  0.008089758455753326       Val Loss:  0.007903257012367248     Train Accuracy:  0.34663783482142857      Validation Accuracy:  0.33910550458715594\n",
            "Epoch:  19     Train Loss:  0.00697413831949234       Val Loss:  0.007859805226325988     Train Accuracy:  0.34486607142857145      Validation Accuracy:  0.3378440366972477\n",
            "Epoch:  20     Train Loss:  0.008045563846826554       Val Loss:  0.00886169821023941     Train Accuracy:  0.355859375      Validation Accuracy:  0.34506880733944956\n",
            "Epoch:  21     Train Loss:  0.00822608694434166       Val Loss:  0.008720614761114121     Train Accuracy:  0.34614955357142857      Validation Accuracy:  0.3386467889908257\n",
            "Epoch:  22     Train Loss:  0.0078037850558757785       Val Loss:  0.007402705401182175     Train Accuracy:  0.34432198660714286      Validation Accuracy:  0.33715596330275227\n",
            "Epoch:  23     Train Loss:  0.0080133818089962       Val Loss:  0.007902610301971435     Train Accuracy:  0.34458705357142855      Validation Accuracy:  0.3373279816513762\n",
            "Epoch:  24     Train Loss:  0.00789768323302269       Val Loss:  0.007803941518068314     Train Accuracy:  0.346484375      Validation Accuracy:  0.33836009174311926\n",
            "Epoch:  25     Train Loss:  0.0076116397976875305       Val Loss:  0.007651632279157638     Train Accuracy:  0.34598214285714285      Validation Accuracy:  0.3382454128440367\n",
            "Epoch:  26     Train Loss:  0.007598228752613068       Val Loss:  0.00795421376824379     Train Accuracy:  0.3444196428571429      Validation Accuracy:  0.3360091743119266\n",
            "Epoch:  27     Train Loss:  0.007280425727367401       Val Loss:  0.008195220679044723     Train Accuracy:  0.35990513392857143      Validation Accuracy:  0.34724770642201835\n",
            "Epoch:  28     Train Loss:  0.007333684712648392       Val Loss:  0.007583744078874588     Train Accuracy:  0.34892578125      Validation Accuracy:  0.3397362385321101\n",
            "Epoch:  29     Train Loss:  0.007711586356163025       Val Loss:  0.008300422877073287     Train Accuracy:  0.35044642857142855      Validation Accuracy:  0.3393348623853211\n",
            "Epoch:  30     Train Loss:  0.00730423629283905       Val Loss:  0.0076783433556556705     Train Accuracy:  0.36325334821428573      Validation Accuracy:  0.3499426605504587\n",
            "Epoch:  31     Train Loss:  0.007208795845508575       Val Loss:  0.007610007375478745     Train Accuracy:  0.359375      Validation Accuracy:  0.3452981651376147\n",
            "Epoch:  32     Train Loss:  0.006970494985580444       Val Loss:  0.00747561901807785     Train Accuracy:  0.36428571428571427      Validation Accuracy:  0.34822247706422016\n",
            "Epoch:  33     Train Loss:  0.006651905924081802       Val Loss:  0.0072536073625087735     Train Accuracy:  0.37063337053571427      Validation Accuracy:  0.35240825688073396\n",
            "Epoch:  34     Train Loss:  0.006406953185796737       Val Loss:  0.0071800097823143     Train Accuracy:  0.37978515625      Validation Accuracy:  0.3565366972477064\n",
            "Epoch:  35     Train Loss:  0.006791393458843231       Val Loss:  0.0073766946792602536     Train Accuracy:  0.3890345982142857      Validation Accuracy:  0.3632454128440367\n",
            "Epoch:  36     Train Loss:  0.007207413017749786       Val Loss:  0.006575178354978561     Train Accuracy:  0.4009486607142857      Validation Accuracy:  0.3706422018348624\n",
            "Epoch:  37     Train Loss:  0.006868530809879303       Val Loss:  0.007620774209499359     Train Accuracy:  0.4069614955357143      Validation Accuracy:  0.3783256880733945\n",
            "Epoch:  38     Train Loss:  0.006281931698322296       Val Loss:  0.007779351621866226     Train Accuracy:  0.4221261160714286      Validation Accuracy:  0.3853784403669725\n",
            "Epoch:  39     Train Loss:  0.006366477906703949       Val Loss:  0.007448820769786835     Train Accuracy:  0.43130580357142856      Validation Accuracy:  0.39111238532110093\n",
            "Epoch:  40     Train Loss:  0.007183302193880081       Val Loss:  0.006782310456037522     Train Accuracy:  0.43447265625      Validation Accuracy:  0.39380733944954127\n",
            "Epoch:  41     Train Loss:  0.006426678597927093       Val Loss:  0.007039792090654373     Train Accuracy:  0.45362723214285716      Validation Accuracy:  0.4099197247706422\n",
            "Epoch:  42     Train Loss:  0.007293863594532013       Val Loss:  0.007142042368650436     Train Accuracy:  0.46273716517857144      Validation Accuracy:  0.41961009174311925\n",
            "Epoch:  43     Train Loss:  0.006865864247083664       Val Loss:  0.00680386945605278     Train Accuracy:  0.4517299107142857      Validation Accuracy:  0.40825688073394495\n",
            "Epoch:  44     Train Loss:  0.00684778094291687       Val Loss:  0.007236110419034958     Train Accuracy:  0.47352120535714287      Validation Accuracy:  0.43400229357798165\n",
            "Epoch:  45     Train Loss:  0.006405662000179291       Val Loss:  0.006168751791119575     Train Accuracy:  0.47788783482142855      Validation Accuracy:  0.4325688073394495\n",
            "Epoch:  46     Train Loss:  0.005387841537594795       Val Loss:  0.006642216444015503     Train Accuracy:  0.48000837053571427      Validation Accuracy:  0.430848623853211\n",
            "Epoch:  47     Train Loss:  0.006347305327653885       Val Loss:  0.00721236988902092     Train Accuracy:  0.4782924107142857      Validation Accuracy:  0.4325688073394495\n",
            "Epoch:  48     Train Loss:  0.006255153566598892       Val Loss:  0.006985839456319809     Train Accuracy:  0.48794642857142856      Validation Accuracy:  0.4399655963302752\n",
            "Epoch:  49     Train Loss:  0.006501940637826919       Val Loss:  0.006195859983563423     Train Accuracy:  0.4890066964285714      Validation Accuracy:  0.4426605504587156\n",
            "Epoch:  50     Train Loss:  0.005834802985191345       Val Loss:  0.006399296224117279     Train Accuracy:  0.49132254464285713      Validation Accuracy:  0.4442660550458716\n",
            "Epoch:  51     Train Loss:  0.005788763239979744       Val Loss:  0.007373020052909851     Train Accuracy:  0.4962890625      Validation Accuracy:  0.45\n",
            "Epoch:  52     Train Loss:  0.005145987123250961       Val Loss:  0.00660860762000084     Train Accuracy:  0.49613560267857143      Validation Accuracy:  0.4485091743119266\n",
            "Epoch:  53     Train Loss:  0.006116031482815742       Val Loss:  0.006416375935077667     Train Accuracy:  0.50068359375      Validation Accuracy:  0.4519495412844037\n",
            "Epoch:  54     Train Loss:  0.0064576961100101474       Val Loss:  0.007092689722776413     Train Accuracy:  0.5001674107142857      Validation Accuracy:  0.4515481651376147\n",
            "Epoch:  55     Train Loss:  0.006039749830961227       Val Loss:  0.006222315505146981     Train Accuracy:  0.5058454241071428      Validation Accuracy:  0.45573394495412844\n",
            "Epoch:  56     Train Loss:  0.005978647619485855       Val Loss:  0.006649390608072281     Train Accuracy:  0.5087053571428571      Validation Accuracy:  0.46376146788990824\n",
            "Epoch:  57     Train Loss:  0.005790441110730171       Val Loss:  0.0059821028262376785     Train Accuracy:  0.5068080357142857      Validation Accuracy:  0.4577408256880734\n",
            "Epoch:  58     Train Loss:  0.0064169630408287045       Val Loss:  0.005836548283696175     Train Accuracy:  0.51328125      Validation Accuracy:  0.46301605504587157\n",
            "Epoch:  59     Train Loss:  0.0064022853970527645       Val Loss:  0.006907649338245392     Train Accuracy:  0.5148995535714286      Validation Accuracy:  0.46680045871559633\n",
            "Epoch:  60     Train Loss:  0.006201041862368584       Val Loss:  0.0063932791352272035     Train Accuracy:  0.5189732142857143      Validation Accuracy:  0.46892201834862385\n",
            "Epoch:  61     Train Loss:  0.006009591370820999       Val Loss:  0.006722920387983322     Train Accuracy:  0.51875      Validation Accuracy:  0.4724770642201835\n",
            "Epoch:  62     Train Loss:  0.005404141545295715       Val Loss:  0.006976526230573654     Train Accuracy:  0.5186383928571429      Validation Accuracy:  0.4705275229357798\n",
            "Epoch:  63     Train Loss:  0.0061678458005189896       Val Loss:  0.006189857050776482     Train Accuracy:  0.5231026785714286      Validation Accuracy:  0.47505733944954126\n",
            "Epoch:  64     Train Loss:  0.004847488924860954       Val Loss:  0.006720585376024246     Train Accuracy:  0.5232561383928571      Validation Accuracy:  0.4770068807339449\n",
            "Epoch:  65     Train Loss:  0.006231148168444634       Val Loss:  0.0067172408103942875     Train Accuracy:  0.5246512276785714      Validation Accuracy:  0.47322247706422016\n",
            "Epoch:  66     Train Loss:  0.006702698767185211       Val Loss:  0.005051763355731964     Train Accuracy:  0.5293666294642857      Validation Accuracy:  0.4797018348623853\n",
            "Epoch:  67     Train Loss:  0.006549803912639618       Val Loss:  0.006720523536205292     Train Accuracy:  0.5295200892857143      Validation Accuracy:  0.47901376146788993\n",
            "Epoch:  68     Train Loss:  0.005906278640031815       Val Loss:  0.006836395710706711     Train Accuracy:  0.5329659598214286      Validation Accuracy:  0.4840596330275229\n",
            "Epoch:  69     Train Loss:  0.006108902022242546       Val Loss:  0.005954936519265175     Train Accuracy:  0.53515625      Validation Accuracy:  0.4834288990825688\n",
            "Epoch:  70     Train Loss:  0.005244448035955429       Val Loss:  0.005596410483121872     Train Accuracy:  0.5369838169642858      Validation Accuracy:  0.48727064220183486\n",
            "Epoch:  71     Train Loss:  0.006028607487678528       Val Loss:  0.006347654759883881     Train Accuracy:  0.5359933035714286      Validation Accuracy:  0.48727064220183486\n",
            "Epoch:  72     Train Loss:  0.005245352908968925       Val Loss:  0.005899082124233246     Train Accuracy:  0.5364536830357143      Validation Accuracy:  0.48297018348623855\n",
            "Epoch:  73     Train Loss:  0.006195820495486259       Val Loss:  0.0067600257694721225     Train Accuracy:  0.5418805803571428      Validation Accuracy:  0.4912270642201835\n",
            "Epoch:  74     Train Loss:  0.005543743446469307       Val Loss:  0.005523041635751724     Train Accuracy:  0.5452566964285714      Validation Accuracy:  0.49208715596330277\n",
            "Epoch:  75     Train Loss:  0.005595973879098892       Val Loss:  0.006170511618256569     Train Accuracy:  0.5441964285714286      Validation Accuracy:  0.49214449541284405\n",
            "Epoch:  76     Train Loss:  0.00607391893863678       Val Loss:  0.005850085616111755     Train Accuracy:  0.5474051339285714      Validation Accuracy:  0.49375\n",
            "Epoch:  77     Train Loss:  0.005305784195661545       Val Loss:  0.005731785669922829     Train Accuracy:  0.5462053571428571      Validation Accuracy:  0.48876146788990826\n",
            "Epoch:  78     Train Loss:  0.00568949468433857       Val Loss:  0.005853429436683655     Train Accuracy:  0.5472795758928571      Validation Accuracy:  0.4910550458715596\n",
            "Epoch:  79     Train Loss:  0.005003697797656059       Val Loss:  0.006188777834177017     Train Accuracy:  0.5496512276785714      Validation Accuracy:  0.48996559633027525\n",
            "Epoch:  80     Train Loss:  0.005536434799432754       Val Loss:  0.005793598666787148     Train Accuracy:  0.5542271205357143      Validation Accuracy:  0.4975917431192661\n",
            "Epoch:  81     Train Loss:  0.005356593057513237       Val Loss:  0.006283077597618103     Train Accuracy:  0.5556640625      Validation Accuracy:  0.5031536697247706\n",
            "Epoch:  82     Train Loss:  0.005252126976847649       Val Loss:  0.006394430994987488     Train Accuracy:  0.5587751116071429      Validation Accuracy:  0.4996559633027523\n",
            "Epoch:  83     Train Loss:  0.005264201015233993       Val Loss:  0.005752091482281685     Train Accuracy:  0.5583844866071429      Validation Accuracy:  0.5001146788990826\n",
            "Epoch:  84     Train Loss:  0.0048762582242488865       Val Loss:  0.005802111700177193     Train Accuracy:  0.5598493303571429      Validation Accuracy:  0.500802752293578\n",
            "Epoch:  85     Train Loss:  0.0053365170955657956       Val Loss:  0.005887876451015473     Train Accuracy:  0.5642857142857143      Validation Accuracy:  0.5044724770642202\n",
            "Epoch:  86     Train Loss:  0.005139176174998284       Val Loss:  0.005716655403375626     Train Accuracy:  0.5665318080357142      Validation Accuracy:  0.5048738532110092\n",
            "Epoch:  87     Train Loss:  0.005192761495709419       Val Loss:  0.006095248833298683     Train Accuracy:  0.5677176339285714      Validation Accuracy:  0.5088876146788991\n",
            "Epoch:  88     Train Loss:  0.005706123635172844       Val Loss:  0.006801526248455048     Train Accuracy:  0.5672991071428571      Validation Accuracy:  0.5045298165137615\n",
            "Epoch:  89     Train Loss:  0.0045689702033996586       Val Loss:  0.006531320512294769     Train Accuracy:  0.5716517857142858      Validation Accuracy:  0.5098050458715596\n",
            "Epoch:  90     Train Loss:  0.005198698490858078       Val Loss:  0.006405451893806457     Train Accuracy:  0.5736746651785715      Validation Accuracy:  0.5129587155963303\n",
            "Epoch:  91     Train Loss:  0.005011335015296936       Val Loss:  0.005103639513254166     Train Accuracy:  0.5748883928571429      Validation Accuracy:  0.5146788990825688\n",
            "Epoch:  92     Train Loss:  0.004587290436029434       Val Loss:  0.005681858956813812     Train Accuracy:  0.576171875      Validation Accuracy:  0.510493119266055\n",
            "Epoch:  93     Train Loss:  0.004721633717417717       Val Loss:  0.006946815550327301     Train Accuracy:  0.5768415178571429      Validation Accuracy:  0.5114678899082569\n",
            "Epoch:  94     Train Loss:  0.005142221599817276       Val Loss:  0.006080808490514756     Train Accuracy:  0.5816685267857142      Validation Accuracy:  0.5184633027522936\n",
            "Epoch:  95     Train Loss:  0.005436111614108086       Val Loss:  0.005309678241610527     Train Accuracy:  0.5792271205357142      Validation Accuracy:  0.5154243119266055\n",
            "Epoch:  96     Train Loss:  0.00453329011797905       Val Loss:  0.005375223606824875     Train Accuracy:  0.5852399553571429      Validation Accuracy:  0.5204701834862385\n",
            "Epoch:  97     Train Loss:  0.004707245901226997       Val Loss:  0.005779578909277916     Train Accuracy:  0.5882393973214286      Validation Accuracy:  0.5222477064220183\n",
            "Epoch:  98     Train Loss:  0.0052158314734697345       Val Loss:  0.0051453918218612674     Train Accuracy:  0.58720703125      Validation Accuracy:  0.5226490825688074\n",
            "Epoch:  99     Train Loss:  0.005421068519353867       Val Loss:  0.004781784862279892     Train Accuracy:  0.5915318080357143      Validation Accuracy:  0.5229357798165137\n",
            "Epoch:  100     Train Loss:  0.004460107907652855       Val Loss:  0.0063478536903858185     Train Accuracy:  0.5934151785714286      Validation Accuracy:  0.5264334862385321\n",
            "Epoch:  101     Train Loss:  0.004989859834313393       Val Loss:  0.006189550831913948     Train Accuracy:  0.5956752232142857      Validation Accuracy:  0.5296444954128441\n",
            "Epoch:  102     Train Loss:  0.00525268092751503       Val Loss:  0.006643721461296081     Train Accuracy:  0.5964285714285714      Validation Accuracy:  0.528841743119266\n",
            "Epoch:  103     Train Loss:  0.004575818777084351       Val Loss:  0.005086154490709305     Train Accuracy:  0.5982142857142857      Validation Accuracy:  0.5298738532110092\n",
            "Epoch:  104     Train Loss:  0.004551800712943077       Val Loss:  0.005797011405229568     Train Accuracy:  0.6007672991071429      Validation Accuracy:  0.5266628440366973\n",
            "Epoch:  105     Train Loss:  0.005006806924939156       Val Loss:  0.005587396770715713     Train Accuracy:  0.6014369419642858      Validation Accuracy:  0.5295298165137615\n",
            "Epoch:  106     Train Loss:  0.004468951374292374       Val Loss:  0.0043480034917593     Train Accuracy:  0.6055803571428572      Validation Accuracy:  0.5388761467889909\n",
            "Epoch:  107     Train Loss:  0.0048251539468765255       Val Loss:  0.0062982551753520966     Train Accuracy:  0.6088169642857143      Validation Accuracy:  0.537901376146789\n",
            "Epoch:  108     Train Loss:  0.004953957349061966       Val Loss:  0.005873068794608116     Train Accuracy:  0.6099748883928572      Validation Accuracy:  0.5402522935779817\n",
            "Epoch:  109     Train Loss:  0.004212146252393722       Val Loss:  0.005861440673470497     Train Accuracy:  0.6125837053571429      Validation Accuracy:  0.538704128440367\n",
            "Epoch:  110     Train Loss:  0.0043775573372840885       Val Loss:  0.0051430176943540575     Train Accuracy:  0.6125139508928571      Validation Accuracy:  0.5394495412844037\n",
            "Epoch:  111     Train Loss:  0.0047548703849315645       Val Loss:  0.00536816269159317     Train Accuracy:  0.617578125      Validation Accuracy:  0.5439220183486239\n",
            "Epoch:  112     Train Loss:  0.00453404076397419       Val Loss:  0.004892378300428391     Train Accuracy:  0.6167829241071429      Validation Accuracy:  0.5444380733944955\n",
            "Epoch:  113     Train Loss:  0.005120387673377991       Val Loss:  0.005403014272451401     Train Accuracy:  0.6172014508928572      Validation Accuracy:  0.5442660550458716\n",
            "Epoch:  114     Train Loss:  0.004421625658869744       Val Loss:  0.005756390839815139     Train Accuracy:  0.6220982142857143      Validation Accuracy:  0.5473050458715596\n",
            "Epoch:  115     Train Loss:  0.005025678128004074       Val Loss:  0.005972994863986969     Train Accuracy:  0.62216796875      Validation Accuracy:  0.5472477064220184\n",
            "Epoch:  116     Train Loss:  0.004035341739654541       Val Loss:  0.004914858192205429     Train Accuracy:  0.6249162946428571      Validation Accuracy:  0.5482798165137615\n",
            "Epoch:  117     Train Loss:  0.003832856938242912       Val Loss:  0.0058742713183164595     Train Accuracy:  0.62783203125      Validation Accuracy:  0.5500573394495413\n",
            "Epoch:  118     Train Loss:  0.004501016065478325       Val Loss:  0.006026763468980789     Train Accuracy:  0.6240094866071428      Validation Accuracy:  0.5467316513761468\n",
            "Epoch:  119     Train Loss:  0.004296085983514786       Val Loss:  0.00558767169713974     Train Accuracy:  0.6324497767857142      Validation Accuracy:  0.5547591743119266\n",
            "Epoch:  120     Train Loss:  0.004782988876104355       Val Loss:  0.005314619094133377     Train Accuracy:  0.6340680803571429      Validation Accuracy:  0.5572821100917431\n",
            "Epoch:  121     Train Loss:  0.004171497747302055       Val Loss:  0.005227132514119148     Train Accuracy:  0.6351841517857143      Validation Accuracy:  0.5575688073394496\n",
            "Epoch:  122     Train Loss:  0.0034468002617359162       Val Loss:  0.005167529731988907     Train Accuracy:  0.6368582589285714      Validation Accuracy:  0.5595756880733945\n",
            "Epoch:  123     Train Loss:  0.004290254041552544       Val Loss:  0.006100929528474808     Train Accuracy:  0.6378487723214286      Validation Accuracy:  0.560493119266055\n",
            "Epoch:  124     Train Loss:  0.003782019019126892       Val Loss:  0.004729103669524193     Train Accuracy:  0.6411411830357143      Validation Accuracy:  0.5631880733944954\n",
            "Epoch:  125     Train Loss:  0.004697690159082413       Val Loss:  0.005745317041873932     Train Accuracy:  0.6425920758928572      Validation Accuracy:  0.5621559633027523\n",
            "Epoch:  126     Train Loss:  0.0030790382996201517       Val Loss:  0.004658631607890129     Train Accuracy:  0.6458705357142858      Validation Accuracy:  0.5647935779816514\n",
            "Epoch:  127     Train Loss:  0.004716939479112625       Val Loss:  0.004456881061196327     Train Accuracy:  0.6457589285714286      Validation Accuracy:  0.5657110091743119\n",
            "Epoch:  128     Train Loss:  0.00422183983027935       Val Loss:  0.005422748997807503     Train Accuracy:  0.6492606026785714      Validation Accuracy:  0.5666284403669725\n",
            "Epoch:  129     Train Loss:  0.004189608618617058       Val Loss:  0.005321639776229859     Train Accuracy:  0.6510463169642857      Validation Accuracy:  0.5678325688073395\n",
            "Epoch:  130     Train Loss:  0.0043618597090244295       Val Loss:  0.004571360349655151     Train Accuracy:  0.6546037946428571      Validation Accuracy:  0.5723050458715596\n",
            "Epoch:  131     Train Loss:  0.0038254059851169585       Val Loss:  0.006012387946248055     Train Accuracy:  0.6556361607142858      Validation Accuracy:  0.5704701834862386\n",
            "Epoch:  132     Train Loss:  0.00450662337243557       Val Loss:  0.005477230995893478     Train Accuracy:  0.6558872767857142      Validation Accuracy:  0.5735091743119266\n",
            "Epoch:  133     Train Loss:  0.004441290348768234       Val Loss:  0.0057261563837528225     Train Accuracy:  0.6600446428571428      Validation Accuracy:  0.5760894495412844\n",
            "Epoch:  134     Train Loss:  0.004577818512916565       Val Loss:  0.00509517602622509     Train Accuracy:  0.6611328125      Validation Accuracy:  0.5791284403669725\n",
            "Epoch:  135     Train Loss:  0.003889763355255127       Val Loss:  0.0049748547375202175     Train Accuracy:  0.6613002232142857      Validation Accuracy:  0.5787844036697247\n",
            "Epoch:  136     Train Loss:  0.0035510964691638945       Val Loss:  0.004637594148516655     Train Accuracy:  0.6661690848214286      Validation Accuracy:  0.5801605504587156\n",
            "Epoch:  137     Train Loss:  0.004155014455318451       Val Loss:  0.00477997288107872     Train Accuracy:  0.6687779017857143      Validation Accuracy:  0.5823394495412844\n",
            "Epoch:  138     Train Loss:  0.0031270265579223635       Val Loss:  0.005976392328739167     Train Accuracy:  0.6689871651785714      Validation Accuracy:  0.5844610091743119\n",
            "Epoch:  139     Train Loss:  0.004032212495803833       Val Loss:  0.0050734240561723706     Train Accuracy:  0.6718191964285715      Validation Accuracy:  0.5842889908256881\n",
            "Epoch:  140     Train Loss:  0.003696424886584282       Val Loss:  0.005316990613937378     Train Accuracy:  0.67314453125      Validation Accuracy:  0.5837155963302753\n",
            "Epoch:  141     Train Loss:  0.003741651773452759       Val Loss:  0.0038055092096328734     Train Accuracy:  0.6759626116071429      Validation Accuracy:  0.5873279816513761\n",
            "Epoch:  142     Train Loss:  0.003735540434718132       Val Loss:  0.00487678200006485     Train Accuracy:  0.6759765625      Validation Accuracy:  0.5890481651376147\n",
            "Epoch:  143     Train Loss:  0.0031353868544101715       Val Loss:  0.004914351180195809     Train Accuracy:  0.67998046875      Validation Accuracy:  0.5905389908256881\n",
            "Epoch:  144     Train Loss:  0.00336906760931015       Val Loss:  0.004866372793912888     Train Accuracy:  0.6818080357142857      Validation Accuracy:  0.5909977064220183\n",
            "Epoch:  145     Train Loss:  0.003718726709485054       Val Loss:  0.004989743605256081     Train Accuracy:  0.68330078125      Validation Accuracy:  0.5928325688073395\n",
            "Epoch:  146     Train Loss:  0.003365727514028549       Val Loss:  0.0047251850366592405     Train Accuracy:  0.6851422991071429      Validation Accuracy:  0.5916857798165137\n",
            "Epoch:  147     Train Loss:  0.002881919965147972       Val Loss:  0.004854457825422287     Train Accuracy:  0.6888532366071428      Validation Accuracy:  0.5948394495412844\n",
            "Epoch:  148     Train Loss:  0.003974305838346482       Val Loss:  0.0048519369214773175     Train Accuracy:  0.6884347098214286      Validation Accuracy:  0.5990252293577981\n",
            "Epoch:  149     Train Loss:  0.0033088531345129013       Val Loss:  0.004661449790000915     Train Accuracy:  0.6934151785714285      Validation Accuracy:  0.600401376146789\n",
            "Epoch:  150     Train Loss:  0.0034970462322235106       Val Loss:  0.005348911136388778     Train Accuracy:  0.6923130580357143      Validation Accuracy:  0.6005160550458716\n",
            "Epoch:  151     Train Loss:  0.003274170681834221       Val Loss:  0.004854622483253479     Train Accuracy:  0.6938058035714286      Validation Accuracy:  0.5975344036697248\n",
            "Epoch:  152     Train Loss:  0.003269501030445099       Val Loss:  0.00472143180668354     Train Accuracy:  0.6987862723214285      Validation Accuracy:  0.6047018348623853\n",
            "Epoch:  153     Train Loss:  0.0027518874034285546       Val Loss:  0.003960108384490013     Train Accuracy:  0.6998744419642857      Validation Accuracy:  0.6048738532110092\n",
            "Epoch:  154     Train Loss:  0.003830396384000778       Val Loss:  0.004889583960175514     Train Accuracy:  0.70087890625      Validation Accuracy:  0.6052178899082569\n",
            "Epoch:  155     Train Loss:  0.0031444460153579714       Val Loss:  0.004948526993393898     Train Accuracy:  0.7041294642857143      Validation Accuracy:  0.6077408256880734\n",
            "Epoch:  156     Train Loss:  0.00322413370013237       Val Loss:  0.004228264093399048     Train Accuracy:  0.7071986607142857      Validation Accuracy:  0.6104931192660551\n",
            "Epoch:  157     Train Loss:  0.0037153240293264387       Val Loss:  0.004038859903812408     Train Accuracy:  0.7071149553571429      Validation Accuracy:  0.6104931192660551\n",
            "Epoch:  158     Train Loss:  0.002979435957968235       Val Loss:  0.003862881660461426     Train Accuracy:  0.7099748883928572      Validation Accuracy:  0.6120412844036697\n",
            "Epoch:  159     Train Loss:  0.004027198627591133       Val Loss:  0.004773076996207238     Train Accuracy:  0.7123325892857143      Validation Accuracy:  0.6129587155963303\n",
            "Epoch:  160     Train Loss:  0.0028340671211481093       Val Loss:  0.005081435292959213     Train Accuracy:  0.7123604910714286      Validation Accuracy:  0.6130160550458715\n",
            "Epoch:  161     Train Loss:  0.003171863406896591       Val Loss:  0.00419408231973648     Train Accuracy:  0.7154575892857142      Validation Accuracy:  0.614506880733945\n",
            "Epoch:  162     Train Loss:  0.003528236225247383       Val Loss:  0.004234421998262405     Train Accuracy:  0.7175641741071429      Validation Accuracy:  0.6186926605504587\n",
            "Epoch:  163     Train Loss:  0.003405674546957016       Val Loss:  0.00418478474020958     Train Accuracy:  0.7185825892857143      Validation Accuracy:  0.6176605504587156\n",
            "Epoch:  164     Train Loss:  0.0032074987888336183       Val Loss:  0.0046343781054019925     Train Accuracy:  0.7210239955357143      Validation Accuracy:  0.6178325688073395\n",
            "Epoch:  165     Train Loss:  0.003537919744849205       Val Loss:  0.005093833059072494     Train Accuracy:  0.7235630580357143      Validation Accuracy:  0.6215022935779817\n",
            "Epoch:  166     Train Loss:  0.0028122523799538612       Val Loss:  0.005014988034963608     Train Accuracy:  0.7245256696428571      Validation Accuracy:  0.6231077981651376\n",
            "Epoch:  167     Train Loss:  0.0030826162546873093       Val Loss:  0.0038207773119211196     Train Accuracy:  0.7257254464285714      Validation Accuracy:  0.6228211009174311\n",
            "Epoch:  168     Train Loss:  0.0031471036374568937       Val Loss:  0.00421312190592289     Train Accuracy:  0.7295200892857143      Validation Accuracy:  0.6272362385321101\n",
            "Epoch:  169     Train Loss:  0.0039374765008687975       Val Loss:  0.004773467034101486     Train Accuracy:  0.7294921875      Validation Accuracy:  0.6266055045871559\n",
            "Epoch:  170     Train Loss:  0.0031699147075414658       Val Loss:  0.0049150407314300535     Train Accuracy:  0.7302455357142857      Validation Accuracy:  0.6255160550458716\n",
            "Epoch:  171     Train Loss:  0.003104373812675476       Val Loss:  0.0035495158284902573     Train Accuracy:  0.7331333705357143      Validation Accuracy:  0.6281536697247706\n",
            "Epoch:  172     Train Loss:  0.002084037661552429       Val Loss:  0.005152035877108574     Train Accuracy:  0.7353934151785714      Validation Accuracy:  0.6311926605504588\n",
            "Epoch:  173     Train Loss:  0.0035797279328107835       Val Loss:  0.004939334839582444     Train Accuracy:  0.7367327008928571      Validation Accuracy:  0.629243119266055\n",
            "Epoch:  174     Train Loss:  0.003407791629433632       Val Loss:  0.004843085631728172     Train Accuracy:  0.7386997767857143      Validation Accuracy:  0.632855504587156\n",
            "Epoch:  175     Train Loss:  0.0033395450562238695       Val Loss:  0.0050614185631275175     Train Accuracy:  0.7413504464285714      Validation Accuracy:  0.6327408256880734\n",
            "Epoch:  176     Train Loss:  0.0035069018602371218       Val Loss:  0.0051088549196720125     Train Accuracy:  0.7419782366071429      Validation Accuracy:  0.6336009174311926\n",
            "Epoch:  177     Train Loss:  0.002653268538415432       Val Loss:  0.003985971212387085     Train Accuracy:  0.7422991071428572      Validation Accuracy:  0.6335435779816514\n",
            "Epoch:  178     Train Loss:  0.00271749347448349       Val Loss:  0.004372389614582061     Train Accuracy:  0.7448102678571429      Validation Accuracy:  0.6362958715596331\n",
            "Epoch:  179     Train Loss:  0.0029762495309114457       Val Loss:  0.004065636917948723     Train Accuracy:  0.7467494419642857      Validation Accuracy:  0.6383600917431193\n",
            "Epoch:  180     Train Loss:  0.002589844539761543       Val Loss:  0.003935472294688225     Train Accuracy:  0.7475167410714286      Validation Accuracy:  0.6380160550458716\n",
            "Epoch:  181     Train Loss:  0.0025103967636823654       Val Loss:  0.004807130992412567     Train Accuracy:  0.7501674107142857      Validation Accuracy:  0.6414564220183486\n",
            "Epoch:  182     Train Loss:  0.002874615229666233       Val Loss:  0.004064885154366493     Train Accuracy:  0.7509626116071428      Validation Accuracy:  0.6411697247706422\n",
            "Epoch:  183     Train Loss:  0.003194856271147728       Val Loss:  0.004719199612736702     Train Accuracy:  0.7520647321428572      Validation Accuracy:  0.6412844036697247\n",
            "Epoch:  184     Train Loss:  0.0030585918575525284       Val Loss:  0.0038536775857210158     Train Accuracy:  0.7553850446428572      Validation Accuracy:  0.6440940366972477\n",
            "Epoch:  185     Train Loss:  0.002809648774564266       Val Loss:  0.004635992646217346     Train Accuracy:  0.7558314732142857      Validation Accuracy:  0.6444380733944954\n",
            "Epoch:  186     Train Loss:  0.003115301765501499       Val Loss:  0.0053417693823575975     Train Accuracy:  0.7586495535714286      Validation Accuracy:  0.6462729357798165\n",
            "Epoch:  187     Train Loss:  0.003172767907381058       Val Loss:  0.004547806829214096     Train Accuracy:  0.7600446428571429      Validation Accuracy:  0.6484518348623853\n",
            "Epoch:  188     Train Loss:  0.002459239587187767       Val Loss:  0.0037012074142694475     Train Accuracy:  0.7611467633928571      Validation Accuracy:  0.647591743119266\n",
            "Epoch:  189     Train Loss:  0.0025469589978456496       Val Loss:  0.0038470931351184843     Train Accuracy:  0.7615234375      Validation Accuracy:  0.6463302752293578\n",
            "Epoch:  190     Train Loss:  0.002027600631117821       Val Loss:  0.003827175498008728     Train Accuracy:  0.7630580357142858      Validation Accuracy:  0.6489105504587156\n",
            "Epoch:  191     Train Loss:  0.0029471486806869505       Val Loss:  0.0030384710058569907     Train Accuracy:  0.7648995535714286      Validation Accuracy:  0.6530963302752294\n",
            "Epoch:  192     Train Loss:  0.002861687168478966       Val Loss:  0.004775728285312653     Train Accuracy:  0.7668805803571429      Validation Accuracy:  0.6514908256880734\n",
            "Epoch:  193     Train Loss:  0.0031328156590461733       Val Loss:  0.0032291349023580553     Train Accuracy:  0.76787109375      Validation Accuracy:  0.6533830275229358\n",
            "Epoch:  194     Train Loss:  0.002669103816151619       Val Loss:  0.0040843643248081206     Train Accuracy:  0.769921875      Validation Accuracy:  0.6533830275229358\n",
            "Epoch:  195     Train Loss:  0.003203238174319267       Val Loss:  0.004675018414855003     Train Accuracy:  0.7698800223214286      Validation Accuracy:  0.656651376146789\n",
            "Epoch:  196     Train Loss:  0.00242751557379961       Val Loss:  0.0037387870252132416     Train Accuracy:  0.7722237723214286      Validation Accuracy:  0.6581995412844037\n",
            "Epoch:  197     Train Loss:  0.0024832148104906083       Val Loss:  0.004298057407140732     Train Accuracy:  0.7742606026785714      Validation Accuracy:  0.658084862385321\n",
            "Epoch:  198     Train Loss:  0.002224420942366123       Val Loss:  0.0046162791550159454     Train Accuracy:  0.7768275669642857      Validation Accuracy:  0.6584862385321101\n",
            "Epoch:  199     Train Loss:  0.0026627954095602034       Val Loss:  0.004371138289570808     Train Accuracy:  0.7774553571428572      Validation Accuracy:  0.6594610091743119\n",
            "Epoch:  200     Train Loss:  0.002286248281598091       Val Loss:  0.003975861892104149     Train Accuracy:  0.7778599330357143      Validation Accuracy:  0.6622706422018348\n",
            "Epoch:  201     Train Loss:  0.002358684316277504       Val Loss:  0.0036255016922950743     Train Accuracy:  0.7798688616071429      Validation Accuracy:  0.6610665137614679\n",
            "Epoch:  202     Train Loss:  0.002564646303653717       Val Loss:  0.00438898466527462     Train Accuracy:  0.7823381696428572      Validation Accuracy:  0.664105504587156\n",
            "Epoch:  203     Train Loss:  0.001861542835831642       Val Loss:  0.0038144391030073164     Train Accuracy:  0.78427734375      Validation Accuracy:  0.6664564220183486\n",
            "Epoch:  204     Train Loss:  0.0024603238329291345       Val Loss:  0.005144283920526504     Train Accuracy:  0.7846819196428572      Validation Accuracy:  0.6648509174311926\n",
            "Epoch:  205     Train Loss:  0.0021062277257442473       Val Loss:  0.004302191361784935     Train Accuracy:  0.7845842633928571      Validation Accuracy:  0.667144495412844\n",
            "Epoch:  206     Train Loss:  0.002512061968445778       Val Loss:  0.004413554817438126     Train Accuracy:  0.7879045758928571      Validation Accuracy:  0.6658830275229358\n",
            "Epoch:  207     Train Loss:  0.0021281085908412935       Val Loss:  0.003331632167100906     Train Accuracy:  0.7882952008928571      Validation Accuracy:  0.6697821100917432\n",
            "Epoch:  208     Train Loss:  0.0023315440863370895       Val Loss:  0.004198455810546875     Train Accuracy:  0.79052734375      Validation Accuracy:  0.6694954128440367\n",
            "Epoch:  209     Train Loss:  0.00232236310839653       Val Loss:  0.0034879516810178758     Train Accuracy:  0.7918108258928571      Validation Accuracy:  0.6706995412844037\n",
            "Epoch:  210     Train Loss:  0.0019978230819106104       Val Loss:  0.0031194819137454035     Train Accuracy:  0.7933314732142858      Validation Accuracy:  0.6734518348623854\n",
            "Epoch:  211     Train Loss:  0.002369518578052521       Val Loss:  0.0029607390984892844     Train Accuracy:  0.7935407366071429      Validation Accuracy:  0.6708715596330275\n",
            "Epoch:  212     Train Loss:  0.0019133754074573516       Val Loss:  0.003881154954433441     Train Accuracy:  0.7941824776785714      Validation Accuracy:  0.6728211009174312\n",
            "Epoch:  213     Train Loss:  0.003526192530989647       Val Loss:  0.0043038282543420795     Train Accuracy:  0.7962751116071428      Validation Accuracy:  0.6744839449541284\n",
            "Epoch:  214     Train Loss:  0.002141168154776096       Val Loss:  0.004206866770982742     Train Accuracy:  0.7995814732142857      Validation Accuracy:  0.6751720183486238\n",
            "Epoch:  215     Train Loss:  0.0019191328436136245       Val Loss:  0.0054554339498281475     Train Accuracy:  0.7998883928571429      Validation Accuracy:  0.6769495412844037\n",
            "Epoch:  216     Train Loss:  0.0020265735685825346       Val Loss:  0.004005039110779762     Train Accuracy:  0.8023716517857142      Validation Accuracy:  0.679644495412844\n",
            "Epoch:  217     Train Loss:  0.0016220984980463982       Val Loss:  0.0030907027423381804     Train Accuracy:  0.80146484375      Validation Accuracy:  0.6766628440366973\n",
            "Epoch:  218     Train Loss:  0.0023430177941918375       Val Loss:  0.004312240332365036     Train Accuracy:  0.8033063616071429      Validation Accuracy:  0.6768922018348624\n",
            "Epoch:  219     Train Loss:  0.0019110046327114106       Val Loss:  0.004606945812702179     Train Accuracy:  0.8047712053571429      Validation Accuracy:  0.6772362385321101\n",
            "Epoch:  220     Train Loss:  0.002346416749060154       Val Loss:  0.004053635522723198     Train Accuracy:  0.8063616071428571      Validation Accuracy:  0.6801605504587156\n",
            "Epoch:  221     Train Loss:  0.0018218347802758216       Val Loss:  0.0027655532583594324     Train Accuracy:  0.8083844866071429      Validation Accuracy:  0.6805045871559633\n",
            "Epoch:  222     Train Loss:  0.0019119855016469956       Val Loss:  0.0037702523171901704     Train Accuracy:  0.8095145089285715      Validation Accuracy:  0.6828555045871559\n",
            "Epoch:  223     Train Loss:  0.0020702168345451354       Val Loss:  0.0038836140185594557     Train Accuracy:  0.8101283482142857      Validation Accuracy:  0.6827981651376147\n",
            "Epoch:  224     Train Loss:  0.00213518887758255       Val Loss:  0.0035858578979969026     Train Accuracy:  0.8130580357142857      Validation Accuracy:  0.6838302752293578\n",
            "Epoch:  225     Train Loss:  0.0020146746188402175       Val Loss:  0.003593640774488449     Train Accuracy:  0.8144810267857143      Validation Accuracy:  0.6856077981651376\n",
            "Epoch:  226     Train Loss:  0.0018851893022656442       Val Loss:  0.003621215745806694     Train Accuracy:  0.8156668526785714      Validation Accuracy:  0.6872706422018349\n",
            "Epoch:  227     Train Loss:  0.0019196594133973123       Val Loss:  0.00331449918448925     Train Accuracy:  0.8157924107142858      Validation Accuracy:  0.6880733944954128\n",
            "Epoch:  228     Train Loss:  0.002230435237288475       Val Loss:  0.00274304635822773     Train Accuracy:  0.8171875      Validation Accuracy:  0.6873279816513761\n",
            "Epoch:  229     Train Loss:  0.002210781164467335       Val Loss:  0.003681056573987007     Train Accuracy:  0.8181501116071429      Validation Accuracy:  0.6891628440366973\n",
            "Epoch:  230     Train Loss:  0.0017166998237371444       Val Loss:  0.0035466428846120833     Train Accuracy:  0.8188197544642857      Validation Accuracy:  0.6904816513761468\n",
            "Epoch:  231     Train Loss:  0.0020279711112380027       Val Loss:  0.0036772627383470534     Train Accuracy:  0.8197544642857143      Validation Accuracy:  0.6902522935779817\n",
            "Epoch:  232     Train Loss:  0.0023334553465247155       Val Loss:  0.0035633351653814316     Train Accuracy:  0.8233537946428572      Validation Accuracy:  0.6910550458715596\n",
            "Epoch:  233     Train Loss:  0.0015452926978468894       Val Loss:  0.0024206778034567833     Train Accuracy:  0.8235630580357143      Validation Accuracy:  0.6915137614678899\n",
            "Epoch:  234     Train Loss:  0.0019783373922109605       Val Loss:  0.0030971966683864594     Train Accuracy:  0.8246372767857143      Validation Accuracy:  0.6916284403669725\n",
            "Epoch:  235     Train Loss:  0.001648516021668911       Val Loss:  0.0034698359668254852     Train Accuracy:  0.8258091517857142      Validation Accuracy:  0.6957568807339449\n",
            "Epoch:  236     Train Loss:  0.001470169425010681       Val Loss:  0.004505179822444916     Train Accuracy:  0.8278459821428571      Validation Accuracy:  0.6954128440366972\n",
            "Epoch:  237     Train Loss:  0.0016612039878964424       Val Loss:  0.0031130101531744005     Train Accuracy:  0.8287667410714286      Validation Accuracy:  0.6956995412844037\n",
            "Epoch:  238     Train Loss:  0.001824565604329109       Val Loss:  0.003540590777993202     Train Accuracy:  0.8289202008928571      Validation Accuracy:  0.6982798165137615\n",
            "Epoch:  239     Train Loss:  0.0017436418682336807       Val Loss:  0.002994796261191368     Train Accuracy:  0.83203125      Validation Accuracy:  0.6975917431192661\n",
            "Epoch:  240     Train Loss:  0.0014644269831478596       Val Loss:  0.003458958864212036     Train Accuracy:  0.8319614955357143      Validation Accuracy:  0.6970183486238533\n",
            "Epoch:  241     Train Loss:  0.002426789700984955       Val Loss:  0.003748899698257446     Train Accuracy:  0.8335658482142857      Validation Accuracy:  0.6984518348623853\n",
            "Epoch:  242     Train Loss:  0.0018725525587797165       Val Loss:  0.004694830626249313     Train Accuracy:  0.8335100446428572      Validation Accuracy:  0.6995412844036697\n",
            "Epoch:  243     Train Loss:  0.0021198932081460953       Val Loss:  0.003566376492381096     Train Accuracy:  0.8354352678571428      Validation Accuracy:  0.7011467889908257\n",
            "Epoch:  244     Train Loss:  0.0017807092517614365       Val Loss:  0.002967953123152256     Train Accuracy:  0.8354213169642857      Validation Accuracy:  0.7011467889908257\n",
            "Epoch:  245     Train Loss:  0.0017913827672600747       Val Loss:  0.003946047276258469     Train Accuracy:  0.8365931919642857      Validation Accuracy:  0.7006307339449541\n",
            "Epoch:  246     Train Loss:  0.0014453385956585407       Val Loss:  0.003365746513009071     Train Accuracy:  0.8395089285714286      Validation Accuracy:  0.7051605504587156\n",
            "Epoch:  247     Train Loss:  0.001878136582672596       Val Loss:  0.0036838505417108535     Train Accuracy:  0.838671875      Validation Accuracy:  0.7021788990825688\n",
            "Epoch:  248     Train Loss:  0.0016781190410256387       Val Loss:  0.0037105750292539597     Train Accuracy:  0.8421037946428571      Validation Accuracy:  0.7071100917431192\n",
            "Epoch:  249     Train Loss:  0.0016401128843426705       Val Loss:  0.003970658034086227     Train Accuracy:  0.84189453125      Validation Accuracy:  0.7047018348623854\n",
            "Epoch:  250     Train Loss:  0.001505405642092228       Val Loss:  0.0033396072685718536     Train Accuracy:  0.8442243303571428      Validation Accuracy:  0.7077981651376147\n",
            "Epoch:  251     Train Loss:  0.0017204800620675086       Val Loss:  0.0028026485815644264     Train Accuracy:  0.8438755580357142      Validation Accuracy:  0.7072247706422018\n",
            "Epoch:  252     Train Loss:  0.002295779064297676       Val Loss:  0.0035334989428520204     Train Accuracy:  0.8466796875      Validation Accuracy:  0.7106651376146789\n",
            "Epoch:  253     Train Loss:  0.001843973807990551       Val Loss:  0.003938354179263115     Train Accuracy:  0.8480608258928571      Validation Accuracy:  0.708887614678899\n",
            "Epoch:  254     Train Loss:  0.0015569791197776795       Val Loss:  0.003636609762907028     Train Accuracy:  0.8469029017857143      Validation Accuracy:  0.7110091743119266\n",
            "Epoch:  255     Train Loss:  0.0019073182716965674       Val Loss:  0.004045847430825233     Train Accuracy:  0.84921875      Validation Accuracy:  0.7116399082568807\n",
            "Epoch:  256     Train Loss:  0.0009673023596405983       Val Loss:  0.002772390842437744     Train Accuracy:  0.8511439732142857      Validation Accuracy:  0.7134174311926605\n",
            "Epoch:  257     Train Loss:  0.0018951397389173508       Val Loss:  0.00272595901042223     Train Accuracy:  0.8532505580357143      Validation Accuracy:  0.7150802752293578\n",
            "Epoch:  258     Train Loss:  0.001594094932079315       Val Loss:  0.0029192233458161356     Train Accuracy:  0.8524553571428571      Validation Accuracy:  0.7149655963302752\n",
            "Epoch:  259     Train Loss:  0.0012758848257362842       Val Loss:  0.004358167946338654     Train Accuracy:  0.8544642857142857      Validation Accuracy:  0.7155963302752294\n",
            "Epoch:  260     Train Loss:  0.0014730840921401978       Val Loss:  0.005042427778244018     Train Accuracy:  0.8546456473214286      Validation Accuracy:  0.7157683486238532\n",
            "Epoch:  261     Train Loss:  0.0011488215997815133       Val Loss:  0.002663986384868622     Train Accuracy:  0.85595703125      Validation Accuracy:  0.7153096330275229\n",
            "Epoch:  262     Train Loss:  0.0013985218480229377       Val Loss:  0.0031489294022321703     Train Accuracy:  0.85771484375      Validation Accuracy:  0.7145642201834862\n",
            "Epoch:  263     Train Loss:  0.0016392569988965989       Val Loss:  0.0030264144763350485     Train Accuracy:  0.8582449776785714      Validation Accuracy:  0.7200688073394496\n",
            "Epoch:  264     Train Loss:  0.0013208839111030102       Val Loss:  0.0022760301828384398     Train Accuracy:  0.8603236607142857      Validation Accuracy:  0.719552752293578\n",
            "Epoch:  265     Train Loss:  0.0016294814646244048       Val Loss:  0.004464555159211158     Train Accuracy:  0.8605050223214286      Validation Accuracy:  0.721158256880734\n",
            "Epoch:  266     Train Loss:  0.0013043684884905815       Val Loss:  0.003451409935951233     Train Accuracy:  0.86044921875      Validation Accuracy:  0.7196674311926605\n",
            "Epoch:  267     Train Loss:  0.0019149592146277428       Val Loss:  0.0033551760017871855     Train Accuracy:  0.8626116071428571      Validation Accuracy:  0.7215022935779817\n",
            "Epoch:  268     Train Loss:  0.0013470475561916829       Val Loss:  0.0043427996337413784     Train Accuracy:  0.86279296875      Validation Accuracy:  0.7217316513761468\n",
            "Epoch:  269     Train Loss:  0.0015840303152799607       Val Loss:  0.00427534356713295     Train Accuracy:  0.8651227678571428      Validation Accuracy:  0.7235665137614679\n",
            "Epoch:  270     Train Loss:  0.001419589202851057       Val Loss:  0.004593226686120033     Train Accuracy:  0.8644252232142857      Validation Accuracy:  0.719954128440367\n",
            "Epoch:  271     Train Loss:  0.001508706621825695       Val Loss:  0.0028323492035269736     Train Accuracy:  0.8670200892857143      Validation Accuracy:  0.7237385321100918\n",
            "Epoch:  272     Train Loss:  0.0011273574084043503       Val Loss:  0.0028588810935616495     Train Accuracy:  0.8690569196428571      Validation Accuracy:  0.7251146788990825\n",
            "Epoch:  273     Train Loss:  0.0010781098157167436       Val Loss:  0.0020122021436691285     Train Accuracy:  0.8697963169642857      Validation Accuracy:  0.7272362385321101\n",
            "Epoch:  274     Train Loss:  0.0014846508391201496       Val Loss:  0.003561755269765854     Train Accuracy:  0.8718191964285714      Validation Accuracy:  0.7288990825688073\n",
            "Epoch:  275     Train Loss:  0.0015009403228759765       Val Loss:  0.004593536630272866     Train Accuracy:  0.8725725446428572      Validation Accuracy:  0.7264908256880734\n",
            "Epoch:  276     Train Loss:  0.0009732874110341072       Val Loss:  0.003060557506978512     Train Accuracy:  0.8738141741071429      Validation Accuracy:  0.7297018348623853\n",
            "Epoch:  277     Train Loss:  0.001259717158973217       Val Loss:  0.0030216487124562264     Train Accuracy:  0.8750139508928572      Validation Accuracy:  0.7319380733944955\n",
            "Epoch:  278     Train Loss:  0.0015288677997887135       Val Loss:  0.003172959387302399     Train Accuracy:  0.8754185267857143      Validation Accuracy:  0.7310206422018348\n",
            "Epoch:  279     Train Loss:  0.001403919793665409       Val Loss:  0.0033835388720035555     Train Accuracy:  0.8759626116071428      Validation Accuracy:  0.73125\n",
            "Epoch:  280     Train Loss:  0.0018437553197145462       Val Loss:  0.0031252168118953705     Train Accuracy:  0.8773577008928571      Validation Accuracy:  0.7322247706422018\n",
            "Epoch:  281     Train Loss:  0.0015025530941784383       Val Loss:  0.0033673211932182313     Train Accuracy:  0.8777901785714286      Validation Accuracy:  0.7337155963302753\n",
            "Epoch:  282     Train Loss:  0.0013506285846233367       Val Loss:  0.0033292874693870546     Train Accuracy:  0.8805943080357143      Validation Accuracy:  0.734690366972477\n",
            "Epoch:  283     Train Loss:  0.001400643866509199       Val Loss:  0.003267207369208336     Train Accuracy:  0.8799386160714285      Validation Accuracy:  0.7319954128440367\n",
            "Epoch:  284     Train Loss:  0.0013404011726379395       Val Loss:  0.003482348844408989     Train Accuracy:  0.88212890625      Validation Accuracy:  0.736295871559633\n",
            "Epoch:  285     Train Loss:  0.001268041878938675       Val Loss:  0.0028607003390789033     Train Accuracy:  0.8833286830357143      Validation Accuracy:  0.7373279816513761\n",
            "Epoch:  286     Train Loss:  0.0012616327032446862       Val Loss:  0.0027461966499686243     Train Accuracy:  0.8827706473214286      Validation Accuracy:  0.7370412844036697\n",
            "Epoch:  287     Train Loss:  0.0013422292657196521       Val Loss:  0.002180139534175396     Train Accuracy:  0.8849330357142857      Validation Accuracy:  0.7380160550458715\n",
            "Epoch:  288     Train Loss:  0.0014908957295119763       Val Loss:  0.003465486690402031     Train Accuracy:  0.8869698660714286      Validation Accuracy:  0.7404243119266055\n",
            "Epoch:  289     Train Loss:  0.0012854805216193199       Val Loss:  0.004921581968665123     Train Accuracy:  0.8872907366071429      Validation Accuracy:  0.7397935779816514\n",
            "Epoch:  290     Train Loss:  0.00153049323707819       Val Loss:  0.002854301407933235     Train Accuracy:  0.8883928571428571      Validation Accuracy:  0.7407110091743119\n",
            "Epoch:  291     Train Loss:  0.001461116038262844       Val Loss:  0.002687543258070946     Train Accuracy:  0.8903878348214286      Validation Accuracy:  0.7439220183486238\n",
            "Epoch:  292     Train Loss:  0.0009923153556883335       Val Loss:  0.0022505151107907297     Train Accuracy:  0.8907645089285714      Validation Accuracy:  0.7440940366972477\n",
            "Epoch:  293     Train Loss:  0.0008649253286421299       Val Loss:  0.0030470890924334526     Train Accuracy:  0.891796875      Validation Accuracy:  0.744954128440367\n",
            "Epoch:  294     Train Loss:  0.0011490071192383766       Val Loss:  0.0033632792532444     Train Accuracy:  0.8931919642857142      Validation Accuracy:  0.7452408256880734\n",
            "Epoch:  295     Train Loss:  0.000949928630143404       Val Loss:  0.0025398148223757743     Train Accuracy:  0.8938895089285714      Validation Accuracy:  0.7455848623853211\n",
            "Epoch:  296     Train Loss:  0.0014478112570941448       Val Loss:  0.0030736582353711127     Train Accuracy:  0.89521484375      Validation Accuracy:  0.7456422018348624\n",
            "Epoch:  297     Train Loss:  0.0012588177807629109       Val Loss:  0.002036029100418091     Train Accuracy:  0.8946568080357142      Validation Accuracy:  0.7442660550458715\n",
            "Epoch:  298     Train Loss:  0.0011404132470488547       Val Loss:  0.0025478612631559374     Train Accuracy:  0.8970703125      Validation Accuracy:  0.7470756880733945\n",
            "Epoch:  299     Train Loss:  0.0012881508097052573       Val Loss:  0.003378518298268318     Train Accuracy:  0.8988839285714286      Validation Accuracy:  0.7493692660550458\n",
            "Epoch:  300     Train Loss:  0.001473103929311037       Val Loss:  0.0017029820010066032     Train Accuracy:  0.8995256696428572      Validation Accuracy:  0.7506307339449542\n",
            "Epoch:  301     Train Loss:  0.0006807045545428991       Val Loss:  0.002975524589419365     Train Accuracy:  0.9011439732142857      Validation Accuracy:  0.7506880733944954\n",
            "Epoch:  302     Train Loss:  0.0008528227917850018       Val Loss:  0.0024193175137043     Train Accuracy:  0.9012834821428571      Validation Accuracy:  0.7501720183486239\n",
            "Epoch:  303     Train Loss:  0.0008311179466545582       Val Loss:  0.0037385649979114533     Train Accuracy:  0.9025669642857143      Validation Accuracy:  0.7533830275229357\n",
            "Epoch:  304     Train Loss:  0.0011861152946949006       Val Loss:  0.0031531799584627153     Train Accuracy:  0.9006277901785714      Validation Accuracy:  0.7509747706422019\n",
            "Epoch:  305     Train Loss:  0.001254884898662567       Val Loss:  0.002588692493736744     Train Accuracy:  0.9043387276785714      Validation Accuracy:  0.7538990825688073\n",
            "Epoch:  306     Train Loss:  0.0013039794750511647       Val Loss:  0.003120259754359722     Train Accuracy:  0.9056640625      Validation Accuracy:  0.7551032110091743\n",
            "Epoch:  307     Train Loss:  0.0008780266158282757       Val Loss:  0.0030671363696455956     Train Accuracy:  0.9053571428571429      Validation Accuracy:  0.7547591743119266\n",
            "Epoch:  308     Train Loss:  0.0010566690936684609       Val Loss:  0.002876006253063679     Train Accuracy:  0.9071428571428571      Validation Accuracy:  0.7539564220183487\n",
            "Epoch:  309     Train Loss:  0.0010215274058282375       Val Loss:  0.00320635549724102     Train Accuracy:  0.9075613839285714      Validation Accuracy:  0.7553899082568807\n",
            "Epoch:  310     Train Loss:  0.0012146619148552418       Val Loss:  0.0024876348674297334     Train Accuracy:  0.9071149553571428      Validation Accuracy:  0.7556192660550459\n",
            "Epoch:  311     Train Loss:  0.0010221992619335652       Val Loss:  0.0034480661153793333     Train Accuracy:  0.9094029017857143      Validation Accuracy:  0.7564793577981651\n",
            "Epoch:  312     Train Loss:  0.0010412734001874924       Val Loss:  0.003382345288991928     Train Accuracy:  0.9117745535714286      Validation Accuracy:  0.7601490825688073\n",
            "Epoch:  313     Train Loss:  0.0011413251981139184       Val Loss:  0.003738945722579956     Train Accuracy:  0.9110212053571428      Validation Accuracy:  0.7573967889908257\n",
            "Epoch:  314     Train Loss:  0.001186668314039707       Val Loss:  0.001969415508210659     Train Accuracy:  0.9123186383928571      Validation Accuracy:  0.7596330275229358\n",
            "Epoch:  315     Train Loss:  0.0008571291342377663       Val Loss:  0.0028776414692401886     Train Accuracy:  0.9133928571428571      Validation Accuracy:  0.7590022935779817\n",
            "Epoch:  316     Train Loss:  0.000932112243026495       Val Loss:  0.0035882357507944105     Train Accuracy:  0.9153599330357143      Validation Accuracy:  0.7630160550458716\n",
            "Epoch:  317     Train Loss:  0.0007321302779018879       Val Loss:  0.004092520847916603     Train Accuracy:  0.9147879464285714      Validation Accuracy:  0.760493119266055\n",
            "Epoch:  318     Train Loss:  0.0006551702506840229       Val Loss:  0.0024059904739260674     Train Accuracy:  0.9164202008928571      Validation Accuracy:  0.7614105504587156\n",
            "Epoch:  319     Train Loss:  0.0011799675412476063       Val Loss:  0.0026234051212668417     Train Accuracy:  0.91630859375      Validation Accuracy:  0.7611238532110092\n",
            "Epoch:  320     Train Loss:  0.0007331239990890026       Val Loss:  0.0022677069529891012     Train Accuracy:  0.9172154017857143      Validation Accuracy:  0.763302752293578\n",
            "Epoch:  321     Train Loss:  0.001371749769896269       Val Loss:  0.0032761652022600175     Train Accuracy:  0.9186802455357143      Validation Accuracy:  0.7646788990825688\n",
            "Epoch:  322     Train Loss:  0.000702677946537733       Val Loss:  0.002222522720694542     Train Accuracy:  0.9193080357142858      Validation Accuracy:  0.7651376146788991\n",
            "Epoch:  323     Train Loss:  0.0009232035838067532       Val Loss:  0.0039161484688520435     Train Accuracy:  0.92041015625      Validation Accuracy:  0.7652522935779816\n",
            "Epoch:  324     Train Loss:  0.0009022609330713749       Val Loss:  0.0037594221532344816     Train Accuracy:  0.9216517857142857      Validation Accuracy:  0.7676032110091743\n",
            "Epoch:  325     Train Loss:  0.0008946668356657028       Val Loss:  0.0031627628952264784     Train Accuracy:  0.9213309151785715      Validation Accuracy:  0.7651376146788991\n",
            "Epoch:  326     Train Loss:  0.0011246787384152413       Val Loss:  0.0025725960731506348     Train Accuracy:  0.9219587053571429      Validation Accuracy:  0.7655963302752293\n",
            "Epoch:  327     Train Loss:  0.0011508813127875327       Val Loss:  0.004279644042253494     Train Accuracy:  0.9232003348214286      Validation Accuracy:  0.7672018348623854\n",
            "Epoch:  328     Train Loss:  0.0007593519985675812       Val Loss:  0.004867370799183845     Train Accuracy:  0.9235630580357143      Validation Accuracy:  0.7696674311926606\n",
            "Epoch:  329     Train Loss:  0.0009729237295687199       Val Loss:  0.0029067441821098326     Train Accuracy:  0.9247628348214286      Validation Accuracy:  0.7698967889908257\n",
            "Epoch:  330     Train Loss:  0.0009562943130731582       Val Loss:  0.002471626363694668     Train Accuracy:  0.9261160714285714      Validation Accuracy:  0.769151376146789\n",
            "Epoch:  331     Train Loss:  0.0007495465688407421       Val Loss:  0.0024089612066745757     Train Accuracy:  0.9261300223214286      Validation Accuracy:  0.7694380733944954\n",
            "Epoch:  332     Train Loss:  0.0006813198328018188       Val Loss:  0.0021874947473406793     Train Accuracy:  0.927734375      Validation Accuracy:  0.7719610091743119\n",
            "Epoch:  333     Train Loss:  0.0009891735389828682       Val Loss:  0.0037332560867071153     Train Accuracy:  0.9282366071428572      Validation Accuracy:  0.7728211009174312\n",
            "Epoch:  334     Train Loss:  0.0010011393576860427       Val Loss:  0.003716915473341942     Train Accuracy:  0.9290597098214286      Validation Accuracy:  0.7732798165137614\n",
            "Epoch:  335     Train Loss:  0.0008810431696474552       Val Loss:  0.002882855013012886     Train Accuracy:  0.9287527901785714      Validation Accuracy:  0.771559633027523\n",
            "Epoch:  336     Train Loss:  0.0007868835702538491       Val Loss:  0.003238864615559578     Train Accuracy:  0.9301897321428572      Validation Accuracy:  0.7731077981651376\n",
            "Epoch:  337     Train Loss:  0.0006532518658787012       Val Loss:  0.003035966865718365     Train Accuracy:  0.93203125      Validation Accuracy:  0.7755160550458715\n",
            "Epoch:  338     Train Loss:  0.0010378817096352577       Val Loss:  0.0017471672967076302     Train Accuracy:  0.9318359375      Validation Accuracy:  0.776204128440367\n",
            "Epoch:  339     Train Loss:  0.0007777645252645016       Val Loss:  0.0024337379261851312     Train Accuracy:  0.9327566964285714      Validation Accuracy:  0.7768922018348624\n",
            "Epoch:  340     Train Loss:  0.0008132658898830414       Val Loss:  0.002346750907599926     Train Accuracy:  0.9328962053571429      Validation Accuracy:  0.7743119266055046\n",
            "Epoch:  341     Train Loss:  0.0011330725625157355       Val Loss:  0.0031400054693222048     Train Accuracy:  0.9329241071428571      Validation Accuracy:  0.7752866972477064\n",
            "Epoch:  342     Train Loss:  0.0010106938891112805       Val Loss:  0.0023201627656817434     Train Accuracy:  0.9344866071428571      Validation Accuracy:  0.7779243119266055\n",
            "Epoch:  343     Train Loss:  0.0004900416824966669       Val Loss:  0.00266824197024107     Train Accuracy:  0.9364955357142857      Validation Accuracy:  0.7801032110091743\n",
            "Epoch:  344     Train Loss:  0.0008163202553987503       Val Loss:  0.002750318311154842     Train Accuracy:  0.9364397321428571      Validation Accuracy:  0.7793577981651376\n",
            "Epoch:  345     Train Loss:  0.0005576090887188911       Val Loss:  0.0033519573509693144     Train Accuracy:  0.9367466517857143      Validation Accuracy:  0.778841743119266\n",
            "Epoch:  346     Train Loss:  0.0008100015111267567       Val Loss:  0.0026275353506207467     Train Accuracy:  0.9370675223214285      Validation Accuracy:  0.7809059633027523\n",
            "Epoch:  347     Train Loss:  0.0007132458500564099       Val Loss:  0.0026068417355418205     Train Accuracy:  0.9373744419642858      Validation Accuracy:  0.780447247706422\n",
            "Epoch:  348     Train Loss:  0.0009364470839500427       Val Loss:  0.0021200980991125107     Train Accuracy:  0.93935546875      Validation Accuracy:  0.7811353211009174\n",
            "Epoch:  349     Train Loss:  0.0007094022817909718       Val Loss:  0.0027007900178432466     Train Accuracy:  0.9399832589285714      Validation Accuracy:  0.7838876146788991\n",
            "Epoch:  350     Train Loss:  0.0008488954976201057       Val Loss:  0.003530556708574295     Train Accuracy:  0.9401227678571429      Validation Accuracy:  0.7814220183486239\n",
            "Epoch:  351     Train Loss:  0.0007118107285350561       Val Loss:  0.00244865994900465     Train Accuracy:  0.9410295758928572      Validation Accuracy:  0.7837729357798165\n",
            "Epoch:  352     Train Loss:  0.0006184442900121212       Val Loss:  0.0038471318781375884     Train Accuracy:  0.9404157366071428      Validation Accuracy:  0.782454128440367\n",
            "Epoch:  353     Train Loss:  0.0007396283093839884       Val Loss:  0.0018809061497449876     Train Accuracy:  0.9419224330357143      Validation Accuracy:  0.7845756880733945\n",
            "Epoch:  354     Train Loss:  0.000731861824169755       Val Loss:  0.002799041196703911     Train Accuracy:  0.9428152901785715      Validation Accuracy:  0.7840596330275229\n",
            "Epoch:  355     Train Loss:  0.00044324048794806       Val Loss:  0.002234255149960518     Train Accuracy:  0.9441545758928571      Validation Accuracy:  0.7838876146788991\n",
            "Epoch:  356     Train Loss:  0.0008405537344515323       Val Loss:  0.002212679013609886     Train Accuracy:  0.9443359375      Validation Accuracy:  0.7849770642201835\n",
            "Epoch:  357     Train Loss:  0.000549869891256094       Val Loss:  0.0020168261602520944     Train Accuracy:  0.9449776785714286      Validation Accuracy:  0.7864105504587156\n",
            "Epoch:  358     Train Loss:  0.0006794881075620652       Val Loss:  0.003193843737244606     Train Accuracy:  0.9458565848214285      Validation Accuracy:  0.7867545871559632\n",
            "Epoch:  359     Train Loss:  0.0005879898089915514       Val Loss:  0.001010947860777378     Train Accuracy:  0.94521484375      Validation Accuracy:  0.7864678899082569\n",
            "Epoch:  360     Train Loss:  0.0005606298334896564       Val Loss:  0.003670629858970642     Train Accuracy:  0.9473911830357142      Validation Accuracy:  0.7864678899082569\n",
            "Epoch:  361     Train Loss:  0.0007600258104503155       Val Loss:  0.002933854982256889     Train Accuracy:  0.9467912946428572      Validation Accuracy:  0.787901376146789\n",
            "Epoch:  362     Train Loss:  0.00043537220917642114       Val Loss:  0.001317377109080553     Train Accuracy:  0.9477120535714286      Validation Accuracy:  0.7884747706422018\n",
            "Epoch:  363     Train Loss:  0.0005277215503156185       Val Loss:  0.0031272653490304948     Train Accuracy:  0.9483956473214286      Validation Accuracy:  0.7886467889908257\n",
            "Epoch:  364     Train Loss:  0.000733321625739336       Val Loss:  0.0027393335476517677     Train Accuracy:  0.9495256696428571      Validation Accuracy:  0.7885321100917431\n",
            "Epoch:  365     Train Loss:  0.0007922777906060219       Val Loss:  0.0031832512468099594     Train Accuracy:  0.9502371651785714      Validation Accuracy:  0.789908256880734\n",
            "Epoch:  366     Train Loss:  0.0008085199631750583       Val Loss:  0.0019638100638985634     Train Accuracy:  0.9508928571428571      Validation Accuracy:  0.7905389908256881\n",
            "Epoch:  367     Train Loss:  0.0006768194027245045       Val Loss:  0.0025497158989310265     Train Accuracy:  0.9508231026785714      Validation Accuracy:  0.7898509174311926\n",
            "Epoch:  368     Train Loss:  0.0007208874914795161       Val Loss:  0.0027579836547374724     Train Accuracy:  0.9515485491071428      Validation Accuracy:  0.789908256880734\n",
            "Epoch:  369     Train Loss:  0.00066511956974864       Val Loss:  0.0027040995657444     Train Accuracy:  0.95234375      Validation Accuracy:  0.7928899082568808\n",
            "Epoch:  370     Train Loss:  0.0007079592440277338       Val Loss:  0.00278602447360754     Train Accuracy:  0.9519112723214286      Validation Accuracy:  0.791743119266055\n",
            "Epoch:  371     Train Loss:  0.0008101754821836949       Val Loss:  0.0037859994918107986     Train Accuracy:  0.95234375      Validation Accuracy:  0.7918577981651376\n",
            "Epoch:  372     Train Loss:  0.0005175488069653511       Val Loss:  0.0024339742958545685     Train Accuracy:  0.9534737723214286      Validation Accuracy:  0.7926032110091743\n",
            "Epoch:  373     Train Loss:  0.0005634848959743977       Val Loss:  0.003578955680131912     Train Accuracy:  0.9542131696428572      Validation Accuracy:  0.7940940366972477\n",
            "Epoch:  374     Train Loss:  0.0006771299988031388       Val Loss:  0.0025969309732317924     Train Accuracy:  0.9554966517857143      Validation Accuracy:  0.7930619266055046\n",
            "Epoch:  375     Train Loss:  0.0006848313845694065       Val Loss:  0.003151668608188629     Train Accuracy:  0.9553013392857143      Validation Accuracy:  0.7946674311926606\n",
            "Epoch:  376     Train Loss:  0.0008298357948660851       Val Loss:  0.0031607240438461305     Train Accuracy:  0.9552455357142857      Validation Accuracy:  0.7946100917431193\n",
            "Epoch:  377     Train Loss:  0.0006178889889270067       Val Loss:  0.002987724915146828     Train Accuracy:  0.9560825892857143      Validation Accuracy:  0.7935206422018348\n",
            "Epoch:  378     Train Loss:  0.0005022342316806317       Val Loss:  0.0011540899984538555     Train Accuracy:  0.9576869419642857      Validation Accuracy:  0.7962729357798165\n",
            "Epoch:  379     Train Loss:  0.0005713531281799078       Val Loss:  0.002362391911447048     Train Accuracy:  0.95751953125      Validation Accuracy:  0.794151376146789\n",
            "Epoch:  380     Train Loss:  0.0004305692855268717       Val Loss:  0.0009970169514417649     Train Accuracy:  0.9576590401785714      Validation Accuracy:  0.7961009174311927\n",
            "Epoch:  381     Train Loss:  0.000561706954613328       Val Loss:  0.0038951754570007322     Train Accuracy:  0.9584542410714286      Validation Accuracy:  0.796559633027523\n",
            "Epoch:  382     Train Loss:  0.0007257334887981415       Val Loss:  0.002693407982587814     Train Accuracy:  0.95888671875      Validation Accuracy:  0.7950688073394495\n",
            "Epoch:  383     Train Loss:  0.0006358541548252105       Val Loss:  0.003924332186579704     Train Accuracy:  0.9588169642857143      Validation Accuracy:  0.7961582568807339\n",
            "Epoch:  384     Train Loss:  0.0006571086589246989       Val Loss:  0.002105941250920296     Train Accuracy:  0.9598911830357143      Validation Accuracy:  0.7992545871559633\n",
            "Epoch:  385     Train Loss:  0.0004905744921416044       Val Loss:  0.0027678685262799265     Train Accuracy:  0.9606584821428571      Validation Accuracy:  0.7991399082568807\n",
            "Epoch:  386     Train Loss:  0.0005573981907218695       Val Loss:  0.0045099563896656035     Train Accuracy:  0.96162109375      Validation Accuracy:  0.8004587155963303\n",
            "Epoch:  387     Train Loss:  0.000549659552052617       Val Loss:  0.0023812513798475267     Train Accuracy:  0.9625279017857142      Validation Accuracy:  0.799598623853211\n",
            "Epoch:  388     Train Loss:  0.0005658053327351808       Val Loss:  0.001802981086075306     Train Accuracy:  0.9620954241071429      Validation Accuracy:  0.7989678899082568\n",
            "Epoch:  389     Train Loss:  0.0007007404230535031       Val Loss:  0.0035870984196662904     Train Accuracy:  0.9631556919642857      Validation Accuracy:  0.7988532110091743\n",
            "Epoch:  390     Train Loss:  0.0005410250276327133       Val Loss:  0.002427881024777889     Train Accuracy:  0.9629603794642857      Validation Accuracy:  0.8013761467889908\n",
            "Epoch:  391     Train Loss:  0.000633364450186491       Val Loss:  0.00219636857509613     Train Accuracy:  0.9635184151785714      Validation Accuracy:  0.8007454128440367\n",
            "Epoch:  392     Train Loss:  0.0003745571244508028       Val Loss:  0.005101676657795906     Train Accuracy:  0.9636579241071429      Validation Accuracy:  0.8009174311926606\n",
            "Epoch:  393     Train Loss:  0.0005566014908254147       Val Loss:  0.002310767397284508     Train Accuracy:  0.9643275669642857      Validation Accuracy:  0.8026949541284404\n",
            "Epoch:  394     Train Loss:  0.0003582456614822149       Val Loss:  0.004883486777544022     Train Accuracy:  0.964453125      Validation Accuracy:  0.8018922018348624\n",
            "Epoch:  395     Train Loss:  0.0005324449855834245       Val Loss:  0.00390896163880825     Train Accuracy:  0.9654854910714286      Validation Accuracy:  0.8022362385321101\n",
            "Epoch:  396     Train Loss:  0.0005866494029760361       Val Loss:  0.0035526368767023085     Train Accuracy:  0.96533203125      Validation Accuracy:  0.802809633027523\n",
            "Epoch:  397     Train Loss:  0.0005514691118150949       Val Loss:  0.0023383427411317824     Train Accuracy:  0.9662388392857143      Validation Accuracy:  0.8039564220183486\n",
            "Epoch:  398     Train Loss:  0.0006525540258735419       Val Loss:  0.0022154927253723146     Train Accuracy:  0.9664620535714286      Validation Accuracy:  0.8037270642201835\n",
            "Epoch:  399     Train Loss:  0.0006268850062042475       Val Loss:  0.0019410107284784316     Train Accuracy:  0.9669642857142857      Validation Accuracy:  0.8037270642201835\n",
            "Epoch:  400     Train Loss:  0.00039155902341008185       Val Loss:  0.0018851270899176598     Train Accuracy:  0.9670200892857143      Validation Accuracy:  0.8040137614678899\n",
            "Epoch:  401     Train Loss:  0.0005054315086454153       Val Loss:  0.002541973814368248     Train Accuracy:  0.9675641741071429      Validation Accuracy:  0.8024655963302753\n",
            "Epoch:  402     Train Loss:  0.00048044589348137377       Val Loss:  0.0024476196616888047     Train Accuracy:  0.9685686383928571      Validation Accuracy:  0.80625\n",
            "Epoch:  403     Train Loss:  0.0004383918829262257       Val Loss:  0.0006116680335253478     Train Accuracy:  0.9688616071428572      Validation Accuracy:  0.80625\n",
            "Epoch:  404     Train Loss:  0.0005975064821541309       Val Loss:  0.002610085345804691     Train Accuracy:  0.9686383928571428      Validation Accuracy:  0.8068807339449541\n",
            "Epoch:  405     Train Loss:  0.00046378495171666144       Val Loss:  0.0019284198060631752     Train Accuracy:  0.9698381696428572      Validation Accuracy:  0.8056766055045872\n",
            "Epoch:  406     Train Loss:  0.00047367489896714686       Val Loss:  0.0015462663024663926     Train Accuracy:  0.9698102678571429      Validation Accuracy:  0.8080848623853211\n",
            "Epoch:  407     Train Loss:  0.0005613236688077449       Val Loss:  0.001617400161921978     Train Accuracy:  0.9696149553571428      Validation Accuracy:  0.8083715596330275\n",
            "Epoch:  408     Train Loss:  0.0003478658152744174       Val Loss:  0.003198567405343056     Train Accuracy:  0.9712053571428572      Validation Accuracy:  0.805848623853211\n",
            "Epoch:  409     Train Loss:  0.0004458212293684483       Val Loss:  0.0015833420678973199     Train Accuracy:  0.9713588169642857      Validation Accuracy:  0.8094036697247706\n",
            "Epoch:  410     Train Loss:  0.0003148430958390236       Val Loss:  0.0019227810204029082     Train Accuracy:  0.9716657366071428      Validation Accuracy:  0.808256880733945\n",
            "Epoch:  411     Train Loss:  0.0002568112686276436       Val Loss:  0.0027310991659760474     Train Accuracy:  0.9718889508928571      Validation Accuracy:  0.8084288990825688\n",
            "Epoch:  412     Train Loss:  0.0004965657833963632       Val Loss:  0.00337691530585289     Train Accuracy:  0.9725306919642858      Validation Accuracy:  0.8099197247706422\n",
            "Epoch:  413     Train Loss:  0.0005422695074230432       Val Loss:  0.0030001820996403694     Train Accuracy:  0.9724748883928571      Validation Accuracy:  0.8103784403669725\n",
            "Epoch:  414     Train Loss:  0.0005994386039674283       Val Loss:  0.0031168598681688307     Train Accuracy:  0.9727260044642857      Validation Accuracy:  0.810091743119266\n",
            "Epoch:  415     Train Loss:  0.00040740258991718293       Val Loss:  0.0030883518978953362     Train Accuracy:  0.9734654017857143      Validation Accuracy:  0.8091169724770643\n",
            "Epoch:  416     Train Loss:  0.0004847901873290539       Val Loss:  0.002887023240327835     Train Accuracy:  0.9735212053571428      Validation Accuracy:  0.809461009174312\n",
            "Epoch:  417     Train Loss:  0.0003705841023474932       Val Loss:  0.002710236981511116     Train Accuracy:  0.9739118303571429      Validation Accuracy:  0.8107798165137615\n",
            "Epoch:  418     Train Loss:  0.0003305695950984955       Val Loss:  0.003019975312054157     Train Accuracy:  0.9740373883928571      Validation Accuracy:  0.8128440366972477\n",
            "Epoch:  419     Train Loss:  0.0005357526708394289       Val Loss:  0.0020505642518401147     Train Accuracy:  0.9748046875      Validation Accuracy:  0.8118692660550458\n",
            "Epoch:  420     Train Loss:  0.0004554932005703449       Val Loss:  0.0015624737367033958     Train Accuracy:  0.9753208705357143      Validation Accuracy:  0.8110665137614679\n",
            "Epoch:  421     Train Loss:  0.0004718616139143705       Val Loss:  0.0006198565009981394     Train Accuracy:  0.9761858258928572      Validation Accuracy:  0.8124426605504587\n",
            "Epoch:  422     Train Loss:  0.0004385781940072775       Val Loss:  0.0015326857566833497     Train Accuracy:  0.976171875      Validation Accuracy:  0.8117545871559633\n",
            "Epoch:  423     Train Loss:  0.0004757302813231945       Val Loss:  0.003561052307486534     Train Accuracy:  0.97548828125      Validation Accuracy:  0.8130160550458716\n",
            "Epoch:  424     Train Loss:  0.00048491545021533966       Val Loss:  0.002964458428323269     Train Accuracy:  0.9761997767857142      Validation Accuracy:  0.8127866972477065\n",
            "Epoch:  425     Train Loss:  0.0004118234850466251       Val Loss:  0.0024076035246253014     Train Accuracy:  0.9768415178571429      Validation Accuracy:  0.8120412844036697\n",
            "Epoch:  426     Train Loss:  0.00031101733911782504       Val Loss:  0.0035467624664306642     Train Accuracy:  0.9770786830357143      Validation Accuracy:  0.813704128440367\n",
            "Epoch:  427     Train Loss:  0.0004695570562034845       Val Loss:  0.0015193135477602482     Train Accuracy:  0.97666015625      Validation Accuracy:  0.8131307339449542\n",
            "Epoch:  428     Train Loss:  0.0003971233032643795       Val Loss:  0.002992652729153633     Train Accuracy:  0.9777483258928571      Validation Accuracy:  0.8147362385321101\n",
            "Epoch:  429     Train Loss:  0.00045152800157666206       Val Loss:  0.0017564887180924415     Train Accuracy:  0.9778180803571429      Validation Accuracy:  0.8149655963302752\n",
            "Epoch:  430     Train Loss:  0.00021213865838944911       Val Loss:  0.0018741952255368233     Train Accuracy:  0.9783621651785714      Validation Accuracy:  0.8150802752293578\n",
            "Epoch:  431     Train Loss:  0.0004325095098465681       Val Loss:  0.0009962283074855804     Train Accuracy:  0.9785435267857143      Validation Accuracy:  0.8138761467889908\n",
            "Epoch:  432     Train Loss:  0.0004302636720240116       Val Loss:  0.002639867924153805     Train Accuracy:  0.979296875      Validation Accuracy:  0.8152522935779817\n",
            "Epoch:  433     Train Loss:  0.0003185505513101816       Val Loss:  0.0021833769977092745     Train Accuracy:  0.9795340401785714      Validation Accuracy:  0.8147362385321101\n",
            "Epoch:  434     Train Loss:  0.00039968984201550485       Val Loss:  0.0022803891450166704     Train Accuracy:  0.9797572544642857      Validation Accuracy:  0.8141055045871559\n",
            "Epoch:  435     Train Loss:  0.0004266760312020779       Val Loss:  0.0016986813396215438     Train Accuracy:  0.9796456473214286      Validation Accuracy:  0.8156536697247706\n",
            "Epoch:  436     Train Loss:  0.00039400928653776646       Val Loss:  0.0016547635197639466     Train Accuracy:  0.9800502232142857      Validation Accuracy:  0.8154816513761468\n",
            "Epoch:  437     Train Loss:  0.0002578249666839838       Val Loss:  0.000994341168552637     Train Accuracy:  0.9803850446428571      Validation Accuracy:  0.8160550458715596\n",
            "Epoch:  438     Train Loss:  0.0005326085723936558       Val Loss:  0.002772185578942299     Train Accuracy:  0.9804547991071428      Validation Accuracy:  0.8148509174311926\n",
            "Epoch:  439     Train Loss:  0.0003664179472252727       Val Loss:  0.002181456796824932     Train Accuracy:  0.9807059151785714      Validation Accuracy:  0.8162270642201835\n",
            "Epoch:  440     Train Loss:  0.000359867955558002       Val Loss:  0.0032646656036376953     Train Accuracy:  0.9815848214285714      Validation Accuracy:  0.8168004587155964\n",
            "Epoch:  441     Train Loss:  0.0003853090805932879       Val Loss:  0.0030755333602428435     Train Accuracy:  0.981640625      Validation Accuracy:  0.8182912844036697\n",
            "Epoch:  442     Train Loss:  0.00030902151484042407       Val Loss:  0.0030220262706279755     Train Accuracy:  0.98134765625      Validation Accuracy:  0.8171444954128441\n",
            "Epoch:  443     Train Loss:  0.00031292419880628585       Val Loss:  0.0023922350257635115     Train Accuracy:  0.9818359375      Validation Accuracy:  0.8170298165137615\n",
            "Epoch:  444     Train Loss:  0.0004245959222316742       Val Loss:  0.0011927271261811256     Train Accuracy:  0.9816127232142857      Validation Accuracy:  0.8174885321100918\n",
            "Epoch:  445     Train Loss:  0.0003696247236803174       Val Loss:  0.0016725948080420494     Train Accuracy:  0.9825753348214286      Validation Accuracy:  0.8181192660550459\n",
            "Epoch:  446     Train Loss:  0.00034104138612747195       Val Loss:  0.0005305753089487553     Train Accuracy:  0.9828125      Validation Accuracy:  0.8197821100917431\n",
            "Epoch:  447     Train Loss:  0.000346939405426383       Val Loss:  0.0021008295938372613     Train Accuracy:  0.9832310267857143      Validation Accuracy:  0.820355504587156\n",
            "Epoch:  448     Train Loss:  0.0003640228183940053       Val Loss:  0.0021984681487083437     Train Accuracy:  0.9839704241071429      Validation Accuracy:  0.8202408256880734\n",
            "Epoch:  449     Train Loss:  0.0002631747862324119       Val Loss:  0.00282291229814291     Train Accuracy:  0.9838309151785715      Validation Accuracy:  0.819151376146789\n",
            "Epoch:  450     Train Loss:  0.0001834281487390399       Val Loss:  0.0011432697996497155     Train Accuracy:  0.9842215401785714      Validation Accuracy:  0.8208142201834863\n",
            "Epoch:  451     Train Loss:  0.0004249825607985258       Val Loss:  0.0036521367728710175     Train Accuracy:  0.9846819196428571      Validation Accuracy:  0.819954128440367\n",
            "Epoch:  452     Train Loss:  0.0003773365635424852       Val Loss:  0.0012113150209188462     Train Accuracy:  0.9845424107142857      Validation Accuracy:  0.8202981651376147\n",
            "Epoch:  453     Train Loss:  0.0004118169192224741       Val Loss:  0.003891288861632347     Train Accuracy:  0.9845703125      Validation Accuracy:  0.8204128440366972\n",
            "Epoch:  454     Train Loss:  0.00039620655588805674       Val Loss:  0.002352448366582394     Train Accuracy:  0.9845145089285714      Validation Accuracy:  0.8194380733944954\n",
            "Epoch:  455     Train Loss:  0.00033697106409817933       Val Loss:  0.0020853551104664803     Train Accuracy:  0.9853236607142857      Validation Accuracy:  0.8201261467889909\n",
            "Epoch:  456     Train Loss:  0.0004184453282505274       Val Loss:  0.0029563596472144127     Train Accuracy:  0.9846261160714286      Validation Accuracy:  0.8206422018348624\n",
            "Epoch:  457     Train Loss:  0.00029962994158267976       Val Loss:  0.0035065077245235443     Train Accuracy:  0.9849051339285714      Validation Accuracy:  0.8212155963302752\n",
            "Epoch:  458     Train Loss:  0.00036170878447592256       Val Loss:  0.0026476528495550157     Train Accuracy:  0.9855189732142857      Validation Accuracy:  0.8208715596330275\n",
            "Epoch:  459     Train Loss:  0.0002682138700038195       Val Loss:  0.0022694796323776247     Train Accuracy:  0.9855329241071429      Validation Accuracy:  0.8215596330275229\n",
            "Epoch:  460     Train Loss:  0.0003237735480070114       Val Loss:  0.0020014883950352667     Train Accuracy:  0.9860630580357143      Validation Accuracy:  0.8220756880733945\n",
            "Epoch:  461     Train Loss:  0.00034281550906598566       Val Loss:  0.0012098001316189766     Train Accuracy:  0.9861467633928571      Validation Accuracy:  0.8205848623853211\n",
            "Epoch:  462     Train Loss:  0.00016364119946956634       Val Loss:  0.0032601892948150636     Train Accuracy:  0.9864397321428572      Validation Accuracy:  0.8235665137614679\n",
            "Epoch:  463     Train Loss:  0.00024428213946521283       Val Loss:  0.0012141183950006961     Train Accuracy:  0.9863839285714285      Validation Accuracy:  0.8220183486238533\n",
            "Epoch:  464     Train Loss:  0.00026725204661488534       Val Loss:  0.001345610897988081     Train Accuracy:  0.9870256696428571      Validation Accuracy:  0.8235091743119266\n",
            "Epoch:  465     Train Loss:  0.00023404399398714305       Val Loss:  0.0027720922604203223     Train Accuracy:  0.9867745535714286      Validation Accuracy:  0.8234518348623853\n",
            "Epoch:  466     Train Loss:  0.0003587280865758657       Val Loss:  0.0025177009403705595     Train Accuracy:  0.9869280133928572      Validation Accuracy:  0.8239105504587156\n",
            "Epoch:  467     Train Loss:  0.00022450641263276337       Val Loss:  0.0036891188472509386     Train Accuracy:  0.9868582589285714      Validation Accuracy:  0.8230504587155963\n",
            "Epoch:  468     Train Loss:  0.0002398505574092269       Val Loss:  0.003393745794892311     Train Accuracy:  0.98720703125      Validation Accuracy:  0.8224197247706422\n",
            "Epoch:  469     Train Loss:  0.00032860585488379       Val Loss:  0.002123258076608181     Train Accuracy:  0.9874720982142857      Validation Accuracy:  0.8240825688073394\n",
            "Epoch:  470     Train Loss:  0.00019909762777388097       Val Loss:  0.0038185521960258486     Train Accuracy:  0.9875139508928571      Validation Accuracy:  0.8248279816513762\n",
            "Epoch:  471     Train Loss:  0.00021982858888804912       Val Loss:  0.0008881310932338238     Train Accuracy:  0.9871233258928571      Validation Accuracy:  0.8228784403669724\n",
            "Epoch:  472     Train Loss:  0.00032537146471440793       Val Loss:  0.0021244000643491747     Train Accuracy:  0.9878069196428572      Validation Accuracy:  0.8231077981651376\n",
            "Epoch:  473     Train Loss:  0.00034012410324066875       Val Loss:  0.0039859447628259655     Train Accuracy:  0.9879185267857142      Validation Accuracy:  0.8244266055045871\n",
            "Epoch:  474     Train Loss:  0.00034181796945631504       Val Loss:  0.0013922098092734813     Train Accuracy:  0.9883649553571429      Validation Accuracy:  0.8242545871559633\n",
            "Epoch:  475     Train Loss:  0.000222372030839324       Val Loss:  0.001937316171824932     Train Accuracy:  0.9886579241071428      Validation Accuracy:  0.8243119266055046\n",
            "Epoch:  476     Train Loss:  0.00044163675047457216       Val Loss:  0.0029879609122872353     Train Accuracy:  0.9890206473214286      Validation Accuracy:  0.8237958715596331\n",
            "Epoch:  477     Train Loss:  0.00017666909843683243       Val Loss:  0.0010155951604247093     Train Accuracy:  0.9890066964285714      Validation Accuracy:  0.8249426605504587\n",
            "Epoch:  478     Train Loss:  0.00038271560333669186       Val Loss:  0.0028366321697831155     Train Accuracy:  0.98916015625      Validation Accuracy:  0.8258600917431193\n",
            "Epoch:  479     Train Loss:  0.00027884701266884805       Val Loss:  0.0013522760011255742     Train Accuracy:  0.9890904017857143      Validation Accuracy:  0.8248853211009174\n",
            "Epoch:  480     Train Loss:  0.00015875648241490126       Val Loss:  0.0015065468847751617     Train Accuracy:  0.9898995535714286      Validation Accuracy:  0.8260894495412844\n",
            "Epoch:  481     Train Loss:  0.00034413286484777925       Val Loss:  0.003318275138735771     Train Accuracy:  0.98916015625      Validation Accuracy:  0.8251146788990825\n",
            "Epoch:  482     Train Loss:  0.0003138411091640592       Val Loss:  0.001773923821747303     Train Accuracy:  0.98916015625      Validation Accuracy:  0.8249426605504587\n",
            "Epoch:  483     Train Loss:  0.00027448113542050123       Val Loss:  0.002562234736979008     Train Accuracy:  0.9892299107142857      Validation Accuracy:  0.8259747706422018\n",
            "Epoch:  484     Train Loss:  0.00028158295899629595       Val Loss:  0.0029000403359532357     Train Accuracy:  0.9902901785714285      Validation Accuracy:  0.8268922018348623\n",
            "Epoch:  485     Train Loss:  0.0002430336782708764       Val Loss:  0.0022762758657336236     Train Accuracy:  0.9899832589285714      Validation Accuracy:  0.8245412844036697\n",
            "Epoch:  486     Train Loss:  0.00021536778658628463       Val Loss:  0.0012588857673108578     Train Accuracy:  0.9900669642857143      Validation Accuracy:  0.8271788990825688\n",
            "Epoch:  487     Train Loss:  0.0002753082197159529       Val Loss:  0.0033188868314027787     Train Accuracy:  0.9896902901785715      Validation Accuracy:  0.825401376146789\n",
            "Epoch:  488     Train Loss:  0.00015273988246917726       Val Loss:  0.00466746911406517     Train Accuracy:  0.9909458705357143      Validation Accuracy:  0.8270068807339449\n",
            "Epoch:  489     Train Loss:  0.000261020171456039       Val Loss:  0.0024172009900212286     Train Accuracy:  0.9908203125      Validation Accuracy:  0.8274655963302753\n",
            "Epoch:  490     Train Loss:  0.00018004566663876176       Val Loss:  0.0035438954830169676     Train Accuracy:  0.9908063616071429      Validation Accuracy:  0.8274082568807339\n",
            "Epoch:  491     Train Loss:  0.0002986129838973284       Val Loss:  0.0035443428903818132     Train Accuracy:  0.9911830357142857      Validation Accuracy:  0.8263761467889909\n",
            "Epoch:  492     Train Loss:  0.00015973635017871857       Val Loss:  0.0018289536237716675     Train Accuracy:  0.9913783482142857      Validation Accuracy:  0.8294151376146789\n",
            "Epoch:  493     Train Loss:  0.0002969593973830342       Val Loss:  0.0016562636941671372     Train Accuracy:  0.9912388392857143      Validation Accuracy:  0.8275229357798165\n",
            "Epoch:  494     Train Loss:  0.00023912598844617606       Val Loss:  0.002003316208720207     Train Accuracy:  0.9914481026785714      Validation Accuracy:  0.8264334862385321\n",
            "Epoch:  495     Train Loss:  0.0001637923764064908       Val Loss:  0.0017293831333518028     Train Accuracy:  0.9916434151785715      Validation Accuracy:  0.8279816513761468\n",
            "Epoch:  496     Train Loss:  0.00022463009227067233       Val Loss:  0.0022178592160344126     Train Accuracy:  0.9917410714285714      Validation Accuracy:  0.828440366972477\n",
            "Epoch:  497     Train Loss:  0.0002933143638074398       Val Loss:  0.002445919066667557     Train Accuracy:  0.9918805803571429      Validation Accuracy:  0.8269495412844037\n",
            "Epoch:  498     Train Loss:  0.00030630056280642746       Val Loss:  0.0026033123955130576     Train Accuracy:  0.9919782366071429      Validation Accuracy:  0.828440366972477\n",
            "Epoch:  499     Train Loss:  0.00021870783530175686       Val Loss:  0.0032087091356515886     Train Accuracy:  0.9920758928571428      Validation Accuracy:  0.8280963302752293\n",
            "Epoch:  500     Train Loss:  0.00021762847900390624       Val Loss:  0.0016725359484553338     Train Accuracy:  0.9919224330357143      Validation Accuracy:  0.8282110091743119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlIj4f/8r2iSAAtw3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Final Training Accuracy: 0.9919224330357143\n",
            "Final Validation Accuracy: 0.8282110091743119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYfVk9SXBikI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accuracy = get_accuracy_all(model_all, test_data_all, \n",
        "             batch_size = 128, label_type = 0, print_out = False)\n",
        "\n",
        "print(\"Final Test Accuracy: \", test_accuracy*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}